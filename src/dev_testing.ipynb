{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b59f61-6c70-4c37-941e-42bb2bcd6c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fda452b-510f-40c2-9eb6-d834c3b87ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Layker - Dev Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0691f82d-d7d7-45ca-955e-571760f2ab7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9dbee8-d271-41c5-8a95-c5adbe513752",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "validate/yaml.py"
    }
   },
   "outputs": [],
   "source": [
    "# src/layker/snapshot_yaml.py\n",
    "\n",
    "\"\"\"\n",
    "YAML TABLE VALIDATION & SNAPSHOT\n",
    "-------------------------------------------------------------------------------\n",
    "This file validates and sanitizes YAML table DDL for Databricks/Delta Lake.\n",
    "Validation covers all of the following rules:\n",
    "\n",
    "TABLE-LEVEL VALIDATION CHECKS\n",
    "-------------------------------------------------------------------------------\n",
    "- Required top-level keys: catalog, schema, table, columns, properties\n",
    "- catalog/schema/table: must be valid SQL identifiers ([a-z][a-z0-9_]*)\n",
    "- At least one column must be defined under columns\n",
    "- Column keys must be continuous (1,2,...N)\n",
    "- Each column must include: name, datatype, nullable, active\n",
    "- No duplicate column names allowed\n",
    "- Column names must be valid SQL identifiers\n",
    "- datatype must be supported Spark type (or valid complex type)\n",
    "- active must be boolean\n",
    "- If default_value present:\n",
    "    - Must match expected type for that datatype\n",
    "    - Boolean must be bool or string \"true\"/\"false\"\n",
    "- Column comments cannot contain newline, carriage return, tab, or single quote\n",
    "\n",
    "COLUMN CHECK CONSTRAINTS\n",
    "-------------------------------------------------------------------------------\n",
    "- If present, must be a dict\n",
    "- Each constraint must be a dict with both name and expression\n",
    "- No duplicate constraint names per column\n",
    "\n",
    "SCHEMA-LEVEL REFERENCES\n",
    "-------------------------------------------------------------------------------\n",
    "- primary_key, partitioned_by: must reference only existing columns\n",
    "- unique_keys: must be a list of lists, each referencing valid columns\n",
    "- foreign_keys:\n",
    "    - Must be a dict\n",
    "    - Each FK must have: columns, reference_table, reference_columns\n",
    "    - columns must exist\n",
    "    - reference_table must be fully qualified (catalog.schema.table)\n",
    "    - reference_columns must be list of strings\n",
    "\n",
    "TABLE-LEVEL FEATURES\n",
    "-------------------------------------------------------------------------------\n",
    "- table_check_constraints: dict, each with name and expression, no duplicates\n",
    "- row_filters: dict, each with name and expression, no duplicates\n",
    "- tags: must be a dict (if present)\n",
    "- owner: must be string or null (if present)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "# ---- VALIDATOR ----\n",
    "\n",
    "class YamlSnapshot:\n",
    "    REQUIRED_TOP_KEYS = [\"catalog\", \"schema\", \"table\", \"columns\"]\n",
    "    OPTIONAL_TOP_KEYS = [\n",
    "        \"primary_key\", \"partitioned_by\", \"unique_keys\", \"foreign_keys\",\n",
    "        \"table_check_constraints\", \"row_filters\", \"tags\", \"owner\",\n",
    "        \"table_comment\", \"table_properties\"\n",
    "    ]\n",
    "\n",
    "    REQUIRED_COL_KEYS = {\"name\", \"datatype\", \"nullable\", \"active\"}\n",
    "    ALLOWED_OPTIONAL_COL_KEYS = {\n",
    "        \"comment\", \"tags\", \"column_masking_rule\", \"default_value\", \"variable_value\", \"column_check_constraints\"\n",
    "    }\n",
    "\n",
    "    DISALLOWED_COMMENT_CHARS = [\"\\n\", \"\\r\", \"\\t\", \"'\"]\n",
    "\n",
    "    COMPLEX_TYPE_PATTERNS = [\n",
    "        r\"^array<.+>$\", r\"^struct<.+>$\", r\"^map<.+>$\"\n",
    "    ]\n",
    "\n",
    "    ALLOWED_SPARK_TYPES = {\n",
    "        \"string\": str, \"int\": int, \"double\": float, \"float\": float,\n",
    "        \"bigint\": int, \"boolean\": bool, \"binary\": bytes,\n",
    "        \"date\": str, \"timestamp\": str, \"decimal\": float,\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_valid_sql_identifier(name: str) -> bool:\n",
    "        return bool(re.match(r\"^[a-z][a-z0-9_]*$\", name.strip()))\n",
    "\n",
    "    @classmethod\n",
    "    def _is_valid_spark_type(cls, dt: str) -> bool:\n",
    "        dt_lc = dt.lower()\n",
    "        if dt_lc in cls.ALLOWED_SPARK_TYPES:\n",
    "            return True\n",
    "        return any(re.match(p, dt_lc) for p in cls.COMPLEX_TYPE_PATTERNS)\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_fully_qualified_table(ref: str) -> bool:\n",
    "        return ref.count('.') == 2\n",
    "\n",
    "    @classmethod\n",
    "    def validate_dict(cls, cfg: Dict[str, Any]) -> Tuple[bool, List[str]]:\n",
    "        errors: List[str] = []\n",
    "        # 1. Required top-level keys\n",
    "        for key in cls.REQUIRED_TOP_KEYS:\n",
    "            if key not in cfg or cfg[key] in (None, \"\"):\n",
    "                errors.append(f\"Missing top-level key: '{key}'\")\n",
    "\n",
    "        # 2. Check that 'table_comment' and 'table_properties' are *not* under 'properties'\n",
    "        if \"properties\" in cfg:\n",
    "            if isinstance(cfg[\"properties\"], dict):\n",
    "                if \"comment\" in cfg[\"properties\"]:\n",
    "                    errors.append(\"Move 'comment' out of 'properties' and use top-level 'table_comment'\")\n",
    "                if \"table_properties\" in cfg[\"properties\"]:\n",
    "                    errors.append(\"Move 'table_properties' out of 'properties' and use top-level 'table_properties'\")\n",
    "\n",
    "        # 3. Table/catalog/schema identifier validity\n",
    "        for k in (\"catalog\", \"schema\", \"table\"):\n",
    "            v = cfg.get(k, \"\")\n",
    "            if v and not cls._is_valid_sql_identifier(v.replace(\"_\", \"a\").replace(\".\", \"a\")):\n",
    "                errors.append(f\"Invalid {k} name: '{v}'\")\n",
    "\n",
    "        # 4. Columns 1..N\n",
    "        raw = cfg.get(\"columns\", {})\n",
    "        if not raw:\n",
    "            errors.append(\"No columns defined. At least one column is required.\")\n",
    "            cols = {}\n",
    "            nums = []\n",
    "        else:\n",
    "            cols = {str(k): v for k, v in raw.items()}\n",
    "            try:\n",
    "                nums = sorted(map(int, cols.keys()))\n",
    "                if nums != list(range(1, len(nums) + 1)):\n",
    "                    raise ValueError\n",
    "            except Exception:\n",
    "                errors.append(f\"Column keys must be continuous 1..N, got {list(cols.keys())}\")\n",
    "                nums = []\n",
    "\n",
    "        seen_names = set()\n",
    "        all_col_names = []\n",
    "        for i in nums:\n",
    "            col = cols[str(i)]\n",
    "            missing = cls.REQUIRED_COL_KEYS - set(col.keys())\n",
    "            if missing:\n",
    "                errors.append(f\"Column {i} missing keys: {sorted(missing)}\")\n",
    "            name = col.get(\"name\")\n",
    "            if not name or not cls._is_valid_sql_identifier(name):\n",
    "                errors.append(f\"Column {i} name '{name}' invalid\")\n",
    "            if name in seen_names:\n",
    "                errors.append(f\"Duplicate column name: '{name}'\")\n",
    "            seen_names.add(name)\n",
    "            all_col_names.append(name)\n",
    "            dt = col.get(\"datatype\")\n",
    "            if not dt or not cls._is_valid_spark_type(dt):\n",
    "                errors.append(f\"Column {i} datatype '{dt}' not allowed\")\n",
    "            if not isinstance(col.get(\"active\"), bool):\n",
    "                errors.append(f\"Column {i} 'active' must be boolean\")\n",
    "            dv = col.get(\"default_value\")\n",
    "            dt_lc = dt.lower() if dt else \"\"\n",
    "            if dt and dv not in (None, \"\") and dt_lc in cls.ALLOWED_SPARK_TYPES and dt_lc not in (\"date\", \"timestamp\"):\n",
    "                exp = cls.ALLOWED_SPARK_TYPES.get(dt_lc)\n",
    "                if dt_lc == \"boolean\":\n",
    "                    if not isinstance(dv, bool) and not (isinstance(dv, str) and dv.lower() in (\"true\", \"false\")):\n",
    "                        errors.append(f\"Column {i} default '{dv}' invalid for boolean\")\n",
    "                else:\n",
    "                    if not isinstance(dv, exp):\n",
    "                        errors.append(f\"Column {i} default '{dv}' does not match {dt}\")\n",
    "            cm = col.get(\"comment\", \"\")\n",
    "            bad = [ch for ch in cls.DISALLOWED_COMMENT_CHARS if ch in cm]\n",
    "            if bad:\n",
    "                errors.append(f\"Column {i} comment contains {bad}\")\n",
    "\n",
    "            # Column check constraints\n",
    "            ccc = col.get(\"column_check_constraints\", {})\n",
    "            if ccc:\n",
    "                if not isinstance(ccc, dict):\n",
    "                    errors.append(f\"Column {i} column_check_constraints must be a dict\")\n",
    "                else:\n",
    "                    seen_constraint_names = set()\n",
    "                    for cname, cdict in ccc.items():\n",
    "                        if not isinstance(cdict, dict):\n",
    "                            errors.append(f\"Column {i} constraint '{cname}' must be a dict\")\n",
    "                        else:\n",
    "                            if \"name\" not in cdict or \"expression\" not in cdict:\n",
    "                                errors.append(f\"Column {i} constraint '{cname}' missing 'name' or 'expression'\")\n",
    "                            name_val = cdict.get(\"name\")\n",
    "                            if name_val in seen_constraint_names:\n",
    "                                errors.append(f\"Column {i} has duplicate column_check_constraint name '{name_val}'\")\n",
    "                            seen_constraint_names.add(name_val)\n",
    "\n",
    "        def validate_columns_exist(field, value):\n",
    "            for col in value:\n",
    "                if col not in all_col_names:\n",
    "                    errors.append(f\"Field '{field}' references unknown column '{col}'\")\n",
    "\n",
    "        if \"primary_key\" in cfg:\n",
    "            pk = cfg[\"primary_key\"]\n",
    "            pk_cols = pk if isinstance(pk, list) else [pk]\n",
    "            validate_columns_exist(\"primary_key\", pk_cols)\n",
    "        if \"partitioned_by\" in cfg:\n",
    "            pb = cfg[\"partitioned_by\"]\n",
    "            pb_cols = pb if isinstance(pb, list) else [pb]\n",
    "            validate_columns_exist(\"partitioned_by\", pb_cols)\n",
    "        if \"unique_keys\" in cfg:\n",
    "            uk = cfg[\"unique_keys\"]\n",
    "            if not isinstance(uk, list):\n",
    "                errors.append(\"unique_keys must be a list of lists\")\n",
    "            else:\n",
    "                for idx, group in enumerate(uk):\n",
    "                    if not isinstance(group, list):\n",
    "                        errors.append(f\"unique_keys entry {idx} must be a list\")\n",
    "                        continue\n",
    "                    validate_columns_exist(f\"unique_keys[{idx}]\", group)\n",
    "        if \"foreign_keys\" in cfg:\n",
    "            fks = cfg[\"foreign_keys\"]\n",
    "            if not isinstance(fks, dict):\n",
    "                errors.append(\"foreign_keys must be a dict\")\n",
    "            else:\n",
    "                for fk_name, fk in fks.items():\n",
    "                    required_fk_keys = {\"columns\", \"reference_table\", \"reference_columns\"}\n",
    "                    missing_fk = required_fk_keys - set(fk)\n",
    "                    if missing_fk:\n",
    "                        errors.append(f\"Foreign key '{fk_name}' missing keys: {missing_fk}\")\n",
    "                        continue\n",
    "                    validate_columns_exist(f\"foreign_keys.{fk_name}.columns\", fk[\"columns\"])\n",
    "                    ref_tbl = fk[\"reference_table\"]\n",
    "                    if not isinstance(ref_tbl, str) or not cls._is_fully_qualified_table(ref_tbl):\n",
    "                        errors.append(f\"Foreign key '{fk_name}' reference_table '{ref_tbl}' must be fully qualified (catalog.schema.table)\")\n",
    "                    ref_cols = fk[\"reference_columns\"]\n",
    "                    if not isinstance(ref_cols, list) or not all(isinstance(x, str) for x in ref_cols):\n",
    "                        errors.append(f\"Foreign key '{fk_name}' reference_columns must be a list of strings\")\n",
    "        # Table-level check constraints\n",
    "        if \"table_check_constraints\" in cfg:\n",
    "            tcc = cfg[\"table_check_constraints\"]\n",
    "            if not isinstance(tcc, dict):\n",
    "                errors.append(\"table_check_constraints must be a dict\")\n",
    "            else:\n",
    "                names_seen = set()\n",
    "                for cname, cdict in tcc.items():\n",
    "                    if not isinstance(cdict, dict):\n",
    "                        errors.append(f\"table_check_constraints '{cname}' must be a dict\")\n",
    "                        continue\n",
    "                    if \"name\" not in cdict or \"expression\" not in cdict:\n",
    "                        errors.append(f\"table_check_constraints '{cname}' missing 'name' or 'expression'\")\n",
    "                    name_val = cdict.get(\"name\")\n",
    "                    if name_val in names_seen:\n",
    "                        errors.append(f\"Duplicate table_check_constraint name: '{name_val}'\")\n",
    "                    names_seen.add(name_val)\n",
    "        # Row filters\n",
    "        if \"row_filters\" in cfg:\n",
    "            rf = cfg[\"row_filters\"]\n",
    "            if not isinstance(rf, dict):\n",
    "                errors.append(\"row_filters must be a dict\")\n",
    "            else:\n",
    "                names_seen = set()\n",
    "                for fname, fdict in rf.items():\n",
    "                    if not isinstance(fdict, dict):\n",
    "                        errors.append(f\"row_filters '{fname}' must be a dict\")\n",
    "                        continue\n",
    "                    if \"name\" not in fdict or \"expression\" not in fdict:\n",
    "                        errors.append(f\"row_filters '{fname}' missing 'name' or 'expression'\")\n",
    "                    name_val = fdict.get(\"name\")\n",
    "                    if name_val in names_seen:\n",
    "                        errors.append(f\"Duplicate row_filter name: '{name_val}'\")\n",
    "                    names_seen.add(name_val)\n",
    "        if \"tags\" in cfg and not isinstance(cfg[\"tags\"], dict):\n",
    "            errors.append(\"Top-level 'tags' must be a dict\")\n",
    "        if \"owner\" in cfg and not (cfg[\"owner\"] is None or isinstance(cfg[\"owner\"], str)):\n",
    "            errors.append(\"'owner' must be a string or null\")\n",
    "        # Enforce table_comment is string or missing\n",
    "        if \"table_comment\" in cfg and not isinstance(cfg[\"table_comment\"], str):\n",
    "            errors.append(\"'table_comment' must be a string\")\n",
    "        # Enforce table_properties is dict or missing\n",
    "        if \"table_properties\" in cfg and not isinstance(cfg[\"table_properties\"], dict):\n",
    "            errors.append(\"'table_properties' must be a dict\")\n",
    "        return (len(errors) == 0, errors)\n",
    "\n",
    "# ---- SANITIZER ----\n",
    "\n",
    "def sanitize_text(text: Any) -> str:\n",
    "    t = str(text or \"\")\n",
    "    clean = t.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").strip()\n",
    "    return clean.replace(\"'\", \"`\")\n",
    "\n",
    "def recursive_sanitize_comments(obj: Any, path: str = \"\") -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            curr = f\"{path}.{k}\" if path else k\n",
    "            if path.endswith(\".columns\") and isinstance(v, dict) and \"comment\" in v:\n",
    "                if isinstance(v[\"comment\"], str):\n",
    "                    v[\"comment\"] = sanitize_text(v[\"comment\"])\n",
    "            else:\n",
    "                recursive_sanitize_comments(v, curr)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            recursive_sanitize_comments(item, f\"{path}[{i}]\")\n",
    "    return obj\n",
    "\n",
    "def sanitize_metadata(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Table comment: strip lines but preserve newlines\n",
    "    if \"table_comment\" in cfg and isinstance(cfg[\"table_comment\"], str):\n",
    "        lines = str(cfg[\"table_comment\"]).splitlines()\n",
    "        cfg[\"table_comment\"] = \"\\n\".join(line.strip() for line in lines)\n",
    "    # Table properties\n",
    "    if \"table_properties\" in cfg and isinstance(cfg[\"table_properties\"], dict):\n",
    "        for k in list(cfg[\"table_properties\"]):\n",
    "            cfg[\"table_properties\"][k] = sanitize_text(cfg[\"table_properties\"][k])\n",
    "    # Tags\n",
    "    tags = cfg.setdefault(\"tags\", {})\n",
    "    for k in list(tags):\n",
    "        tags[k] = sanitize_text(tags[k])\n",
    "    if \"row_filters\" in cfg:\n",
    "        for rf in cfg[\"row_filters\"].values():\n",
    "            if \"name\" in rf:\n",
    "                rf[\"name\"] = sanitize_text(rf[\"name\"])\n",
    "            if \"expression\" in rf:\n",
    "                rf[\"expression\"] = sanitize_text(rf[\"expression\"])\n",
    "    if \"table_check_constraints\" in cfg:\n",
    "        for c in cfg[\"table_check_constraints\"].values():\n",
    "            if \"name\" in c:\n",
    "                c[\"name\"] = sanitize_text(c[\"name\"])\n",
    "            if \"expression\" in c:\n",
    "                c[\"expression\"] = sanitize_text(c[\"expression\"])\n",
    "    return cfg\n",
    "\n",
    "# ---- SNAPSHOT YAML BUILDER ----\n",
    "\n",
    "def build_snapshot_yaml(cfg: Dict[str, Any], env: Optional[str] = None) -> Tuple[Dict[str, Any], str]:\n",
    "    def _get_catalog():\n",
    "        cat = cfg.get(\"catalog\", \"\").strip()\n",
    "        if cat.endswith(\"_\") and env:\n",
    "            return f\"{cat}{env}\"\n",
    "        return cat\n",
    "\n",
    "    def _get_schema(): return cfg.get(\"schema\", \"\").strip()\n",
    "    def _get_table(): return cfg.get(\"table\", \"\").strip()\n",
    "\n",
    "    fq = f\"{_get_catalog()}.{_get_schema()}.{_get_table()}\"\n",
    "\n",
    "    def _get_tags(): return cfg.get(\"tags\", {})\n",
    "    def _get_comment(): return cfg.get(\"table_comment\", \"\")\n",
    "    def _get_props(): return cfg.get(\"table_properties\", {})\n",
    "\n",
    "    def _get_columns_dict():\n",
    "        cols_dict = cfg.get(\"columns\", {})\n",
    "        cols_dict_str = {str(k): v for k, v in cols_dict.items()}\n",
    "        sorted_keys = sorted(map(int, cols_dict_str.keys()))\n",
    "        col_result = {}\n",
    "        for k in sorted_keys:\n",
    "            col = cols_dict_str[str(k)]\n",
    "            col_result[str(k)] = {\n",
    "                \"name\": col.get(\"name\", \"\"),\n",
    "                \"datatype\": col.get(\"datatype\", \"\"),\n",
    "                \"nullable\": col.get(\"nullable\", True),\n",
    "                \"active\": col.get(\"active\", True),\n",
    "                \"comment\": col.get(\"comment\", \"\"),\n",
    "                \"tags\": col.get(\"tags\", {}),\n",
    "                \"column_masking_rule\": col.get(\"column_masking_rule\", \"\"),\n",
    "                \"column_check_constraints\": col.get(\"column_check_constraints\", {}),\n",
    "            }\n",
    "        return col_result\n",
    "\n",
    "    snapshot = {\n",
    "        \"full_table_name\": fq,\n",
    "        \"catalog\": _get_catalog(),\n",
    "        \"schema\": _get_schema(),\n",
    "        \"table\": _get_table(),\n",
    "        \"primary_key\": cfg.get(\"primary_key\", []),\n",
    "        \"foreign_keys\": cfg.get(\"foreign_keys\", {}),\n",
    "        \"unique_keys\": cfg.get(\"unique_keys\", []),\n",
    "        \"partitioned_by\": cfg.get(\"partitioned_by\", []),\n",
    "        \"table_tags\": _get_tags(),\n",
    "        \"row_filters\": cfg.get(\"row_filters\", {}),\n",
    "        \"table_check_constraints\": cfg.get(\"table_check_constraints\", {}),\n",
    "        \"table_properties\": _get_props(),\n",
    "        \"table_comment\": _get_comment(),\n",
    "        \"owner\": cfg.get(\"owner\", \"\"),\n",
    "        \"columns\": _get_columns_dict(),\n",
    "    }\n",
    "    return snapshot, fq\n",
    "\n",
    "# ---- MAIN ENTRY ----\n",
    "\n",
    "def validate_and_snapshot_yaml(yaml_path: str, env: Optional[str] = None, mode: str = \"all\") -> Tuple[Optional[Dict[str, Any]], Optional[str]]:\n",
    "    # 1. Load YAML\n",
    "    try:\n",
    "        with open(yaml_path, \"r\") as f:\n",
    "            raw_cfg = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"YAML file not found: {yaml_path}\")\n",
    "        sys.exit(2)\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"YAML syntax error in {yaml_path}: {e}\")\n",
    "        sys.exit(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or parsing YAML: {e}\")\n",
    "        sys.exit(2)\n",
    "\n",
    "    # 2. Validate\n",
    "    valid, errors = YamlSnapshot.validate_dict(raw_cfg)\n",
    "    if not valid:\n",
    "        print(\"Validation failed:\")\n",
    "        for err in errors:\n",
    "            print(f\"  - {err}\")\n",
    "        print(\"YAML validation failed. See errors above.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"YAML validation passed.\")\n",
    "\n",
    "    # 3. Sanitize\n",
    "    cfg_clean = recursive_sanitize_comments(raw_cfg)\n",
    "    cfg_clean = sanitize_metadata(cfg_clean)\n",
    "\n",
    "    if mode == \"validate\":\n",
    "        print(\"Validation complete. Exiting after successful validation.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # 4. Build snapshot\n",
    "    snapshot_yaml, fq_table = build_snapshot_yaml(cfg_clean, env=env)\n",
    "    print(\"Snapshot YAML and fully qualified table name are ready.\")\n",
    "    return snapshot_yaml, fq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be068d55-44bf-4ce2-88f0-ada7715ef9ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run -> snapshot_yaml.py"
    }
   },
   "outputs": [],
   "source": [
    "yaml_path = \"layker/resources/test.yaml\"\n",
    "\n",
    "# Call your function and print the output snapshot dict (and FQ table name)\n",
    "snapshot_yaml, fq_table = validate_and_snapshot_yaml(yaml_path, env=None, mode=\"all\")\n",
    "\n",
    "print(\"--- FULLY SANITIZED SNAPSHOT YAML ---\")\n",
    "import pprint\n",
    "pprint.pprint(snapshot_yaml)\n",
    "print(\"--- FULLY QUALIFIED TABLE NAME ---\")\n",
    "print(fq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b05776-ddd3-4208-8f46-62dd9801851f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157eb55e-e4ea-434a-88a8-066421ca5884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88338-81f7-46c0-a5cd-128bf3232674",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Describe Extended Table Configuration"
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "describe_extended_query = \"\"\"\n",
    "DESCRIBE EXTENDED\n",
    "  {table_name}\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(describe_extended_query.format(table_name=table_name)).show(truncate=False, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c43b123b-9155-4688-966c-1436718da3ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table Snapshot"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "############################################################\n",
    "                    Start of First Section                    \n",
    "############################################################\n",
    "\n",
    "# --- SYSTEM TABLE SNAPSHOT QUERIES ---\n",
    "SNAPSHOT_QUERIES = {\n",
    "    \"table_tags\": {\n",
    "        \"table\": \"system.information_schema.table_tags\",\n",
    "        \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"tag_name\", \"tag_value\"],\n",
    "        \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"column_tags\": {\n",
    "        \"table\": \"system.information_schema.column_tags\",\n",
    "        \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"column_name\", \"tag_name\", \"tag_value\"],\n",
    "        \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"row_filters\": {\n",
    "        \"table\": \"system.information_schema.row_filters\",\n",
    "        \"columns\": [\"table_catalog\", \"table_schema\", \"table_name\", \"filter_name\", \"target_columns\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"constraint_table_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_table_usage\",\n",
    "        \"columns\": [\"constraint_catalog\", \"constraint_schema\", \"constraint_name\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"constraint_column_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_column_usage\",\n",
    "        \"columns\": [\"column_name\", \"constraint_name\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "}\n",
    "\n",
    "def parse_fq_table(fq_table: str):\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Expected format: catalog.schema.table\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def build_metadata_sql(kind: str, fq_table: str) -> str:\n",
    "    config = SNAPSHOT_QUERIES[kind]\n",
    "    catalog, schema, table = parse_fq_table(fq_table)\n",
    "    table_vars = [catalog, schema, table]\n",
    "    where_clauses = [\n",
    "        f\"{col_name} = '{table_vars[idx]}'\"\n",
    "        for col_name, idx in config[\"where_keys\"]\n",
    "    ]\n",
    "    columns = \", \".join(config[\"columns\"])\n",
    "    return f\"SELECT {columns} FROM {config['table']} WHERE {' AND '.join(where_clauses)}\"\n",
    "\n",
    "def get_metadata_snapshot(spark: SparkSession, fq_table: str) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    results = {}\n",
    "    for kind in SNAPSHOT_QUERIES:\n",
    "        try:\n",
    "            sql = build_metadata_sql(kind, fq_table)\n",
    "            df = spark.sql(sql)\n",
    "            rows = [row.asDict() for row in df.collect()]\n",
    "            results[kind] = rows\n",
    "        except Exception as e:\n",
    "            results[kind] = f\"[ERROR] {e}\"\n",
    "    return results\n",
    "\n",
    "# --- DESCRIBE TABLE EXTENDED PARSERS ---\n",
    "def get_describe_rows(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    sql = f\"DESCRIBE EXTENDED {fq_table}\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def extract_columns(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    columns = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "        if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "            if col_name == \"# Partition Information\":\n",
    "                break\n",
    "            continue\n",
    "        columns.append({\n",
    "            \"name\": col_name,\n",
    "            \"datatype\": data_type,\n",
    "            \"comment\": comment if comment and comment.upper() != \"NULL\" else \"\",\n",
    "        })\n",
    "    return columns\n",
    "\n",
    "def extract_partitioned_by(describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        if collecting:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name != \"# col_name\":\n",
    "                partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_table_details(describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    details = {}\n",
    "    table_properties = {}\n",
    "    in_details = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Owner\":\n",
    "                details[\"owner\"] = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                details[\"comment\"] = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "    details[\"table_properties\"] = table_properties\n",
    "    return details\n",
    "\n",
    "def extract_constraints(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "    return constraints\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "# --- Example usage, all prints at the bottom: ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Fetch Unity Catalog system metadata (raw output)\n",
    "uc_metadata = get_metadata_snapshot(spark, table_name)\n",
    "\n",
    "# Fetch DESCRIBE EXTENDED rows (raw output)\n",
    "describe_rows = get_describe_rows(spark, table_name)\n",
    "\n",
    "# --- Now print results ---\n",
    "for kind, rows in uc_metadata.items():\n",
    "    print(f\"\\n--- {kind.upper()} ---\")\n",
    "    if isinstance(rows, str) and rows.startswith(\"[ERROR]\"):\n",
    "        print(rows)\n",
    "    elif not rows:\n",
    "        print(\"No rows found.\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "print(\"\\n--- COLUMNS ---\")\n",
    "columns = extract_columns(describe_rows)\n",
    "print(columns if columns else \"No columns found.\")\n",
    "\n",
    "print(\"\\n--- PARTITIONED BY ---\")\n",
    "partitioned_by = extract_partitioned_by(describe_rows)\n",
    "print(partitioned_by if partitioned_by else \"No partitions found.\")\n",
    "\n",
    "print(\"\\n--- TABLE DETAILS ---\")\n",
    "table_details = extract_table_details(describe_rows)\n",
    "print(table_details if table_details else \"No table details found.\")\n",
    "\n",
    "print(\"\\n--- CONSTRAINTS ---\")\n",
    "constraints = extract_constraints(describe_rows)\n",
    "print(constraints if constraints else \"No constraints found.\")\n",
    "\"\"\"\n",
    "\n",
    "############################################################\n",
    "                    End of First Section                    \n",
    "############################################################                \n",
    "\n",
    "\n",
    "# -------------------------------------------------------- #\n",
    "\n",
    "\n",
    "############################################################\n",
    "                    Start of Second Section                    \n",
    "############################################################\n",
    "def build_table_metadata_snapshot(\n",
    "    fq_table: str,\n",
    "    uc_metadata: Dict[str, List[Dict[str, Any]]],\n",
    "    describe_rows: List[Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    catalog, schema, table = parse_fq_table(fq_table)\n",
    "    # Table tags\n",
    "    table_tags = {row[\"tag_name\"]: row[\"tag_value\"] for row in uc_metadata.get(\"table_tags\", [])}\n",
    "    # Table properties, owner, comment\n",
    "    details = extract_table_details(describe_rows)\n",
    "    # Table check constraints (if present in table_properties, or elsewhere)\n",
    "    table_check_constraints = {\n",
    "        k: {\"name\": k, \"expression\": v}\n",
    "        for k, v in details.get(\"table_properties\", {}).items()\n",
    "        if k.startswith(\"delta.constraints\")\n",
    "    }\n",
    "    # Row filters\n",
    "    row_filters = [\n",
    "        {\"filter_name\": row[\"filter_name\"], \"target_columns\": row[\"target_columns\"]}\n",
    "        for row in uc_metadata.get(\"row_filters\", [])\n",
    "    ]\n",
    "    # Partition columns\n",
    "    partitioned_by = extract_partitioned_by(describe_rows)\n",
    "    # Constraints\n",
    "    constraints = extract_constraints(describe_rows)\n",
    "    # Primary key: from constraints\n",
    "    pk = []\n",
    "    for c in constraints:\n",
    "        if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "            m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "            if m:\n",
    "                pk = [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "    # Columns (by index)\n",
    "    columns_raw = extract_columns(describe_rows)\n",
    "    # Column tags (merge by column name)\n",
    "    col_tag_lookup = {}\n",
    "    for row in uc_metadata.get(\"column_tags\", []):\n",
    "        col = row[\"column_name\"]\n",
    "        if col not in col_tag_lookup:\n",
    "            col_tag_lookup[col] = {}\n",
    "        col_tag_lookup[col][row[\"tag_name\"]] = row[\"tag_value\"]\n",
    "    # Column check constraints (by constraint_column_usage)\n",
    "    col_constraint_lookup = {}\n",
    "    for row in uc_metadata.get(\"constraint_column_usage\", []):\n",
    "        col = row[\"column_name\"]\n",
    "        cons = row[\"constraint_name\"]\n",
    "        if col not in col_constraint_lookup:\n",
    "            col_constraint_lookup[col] = {}\n",
    "        col_constraint_lookup[col][cons] = {\"name\": cons}  # Expression requires deeper parsing if needed\n",
    "\n",
    "    # Build columns dictionary by position (1-based, as in your spec)\n",
    "    columns = {}\n",
    "    for idx, col in enumerate(columns_raw, start=1):\n",
    "        colname = col[\"name\"]\n",
    "        columns[idx] = {\n",
    "            \"column_name\": colname,\n",
    "            \"datatype\": col[\"datatype\"],\n",
    "            \"comment\": col[\"comment\"],\n",
    "            \"nullable\": None,  # Could be extracted if needed\n",
    "            \"masking_rule\": None,  # Could be extracted if needed\n",
    "            \"column_tags\": col_tag_lookup.get(colname, {}),\n",
    "            \"column_check_constraints\": col_constraint_lookup.get(colname, {}),\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        \"table\": {\n",
    "            \"fully_qualified_name\": fq_table,\n",
    "            \"catalog\": catalog,\n",
    "            \"schema\": schema,\n",
    "            \"table\": table,\n",
    "            \"owner\": details.get(\"owner\", \"\"),\n",
    "            \"comment\": details.get(\"comment\", \"\"),\n",
    "            \"table_properties\": details.get(\"table_properties\", {}),\n",
    "            \"table_tags\": table_tags,\n",
    "            \"table_check_constraints\": table_check_constraints,\n",
    "            \"row_filters\": row_filters,\n",
    "            \"partitioned_by\": partitioned_by,\n",
    "            \"primary_key\": pk,\n",
    "            \"columns\": columns,\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# --- Usage Example ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "uc_metadata = get_metadata_snapshot(spark, table_name)\n",
    "describe_rows = get_describe_rows(spark, table_name)\n",
    "\n",
    "snapshot = build_table_metadata_snapshot(table_name, uc_metadata, describe_rows)\n",
    "import pprint; pprint.pprint(snapshot, width=120)\n",
    "\n",
    "############################################################\n",
    "                    End of Second Section                    \n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce092be-7dfe-4fe4-b3b4-d7ca56f9295d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "snapshot_table.py"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class TableSnapshot:\n",
    "    SNAPSHOT_QUERIES = {\n",
    "        \"table_tags\": {\n",
    "            \"table\": \"system.information_schema.table_tags\",\n",
    "            \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"tag_name\", \"tag_value\"],\n",
    "            \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"column_tags\": {\n",
    "            \"table\": \"system.information_schema.column_tags\",\n",
    "            \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"column_name\", \"tag_name\", \"tag_value\"],\n",
    "            \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"row_filters\": {\n",
    "            \"table\": \"system.information_schema.row_filters\",\n",
    "            \"columns\": [\"table_catalog\", \"table_schema\", \"table_name\", \"filter_name\", \"target_columns\"],\n",
    "            \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"constraint_table_usage\": {\n",
    "            \"table\": \"system.information_schema.constraint_table_usage\",\n",
    "            \"columns\": [\"constraint_catalog\", \"constraint_schema\", \"constraint_name\"],\n",
    "            \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"constraint_column_usage\": {\n",
    "            \"table\": \"system.information_schema.constraint_column_usage\",\n",
    "            \"columns\": [\"column_name\", \"constraint_name\"],\n",
    "            \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, spark: SparkSession, fq_table: str):\n",
    "        self.spark = spark\n",
    "        self.fq_table = fq_table\n",
    "        self.catalog, self.schema, self.table = self._parse_fq_table(fq_table)\n",
    "\n",
    "    def _parse_fq_table(self, fq_table: str):\n",
    "        parts = fq_table.split(\".\")\n",
    "        if len(parts) != 3:\n",
    "            raise ValueError(\"Expected format: catalog.schema.table\")\n",
    "        return parts[0], parts[1], parts[2]\n",
    "\n",
    "    def _build_metadata_sql(self, kind: str) -> str:\n",
    "        config = self.SNAPSHOT_QUERIES[kind]\n",
    "        table_vars = [self.catalog, self.schema, self.table]\n",
    "        where_clauses = [\n",
    "            f\"{col_name} = '{table_vars[idx]}'\"\n",
    "            for col_name, idx in config[\"where_keys\"]\n",
    "        ]\n",
    "        columns = \", \".join(config[\"columns\"])\n",
    "        return f\"SELECT {columns} FROM {config['table']} WHERE {' AND '.join(where_clauses)}\"\n",
    "\n",
    "    def _get_metadata_snapshot(self) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        results = {}\n",
    "        for kind in self.SNAPSHOT_QUERIES:\n",
    "            try:\n",
    "                sql = self._build_metadata_sql(kind)\n",
    "                df = self.spark.sql(sql)\n",
    "                results[kind] = [row.asDict() for row in df.collect()]\n",
    "            except Exception:\n",
    "                results[kind] = []\n",
    "        return results\n",
    "\n",
    "    def _get_describe_rows(self) -> List[Dict[str, Any]]:\n",
    "        sql = f\"DESCRIBE EXTENDED {self.fq_table}\"\n",
    "        df = self.spark.sql(sql)\n",
    "        return [row.asDict() for row in df.collect()]\n",
    "\n",
    "    def _extract_columns(self, describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        columns = []\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "            comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else \"\"\n",
    "            if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "                if col_name == \"# Partition Information\":\n",
    "                    break\n",
    "                continue\n",
    "            columns.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comment if comment.upper() != \"NULL\" else \"\",\n",
    "            })\n",
    "        return columns\n",
    "\n",
    "    def _extract_partitioned_by(self, describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "        # More robust: handle extra headers, blank lines, and section switches.\n",
    "        collecting = False\n",
    "        partition_cols = []\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            # Start block\n",
    "            if col_name == \"# Partition Information\":\n",
    "                collecting = True\n",
    "                continue\n",
    "            if collecting:\n",
    "                if col_name.startswith(\"#\") and col_name != \"# col_name\":\n",
    "                    break  # new section begins\n",
    "                if col_name == \"\" or col_name == \"# col_name\":\n",
    "                    continue  # skip headers/empties\n",
    "                partition_cols.append(col_name)\n",
    "        return partition_cols\n",
    "\n",
    "    def _extract_table_details(self, describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        details = {}\n",
    "        table_properties = {}\n",
    "        in_details = False\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "            if col_name == \"# Detailed Table Information\":\n",
    "                in_details = True\n",
    "                continue\n",
    "            if in_details:\n",
    "                if not col_name or col_name.startswith(\"#\"):\n",
    "                    break\n",
    "                if col_name == \"Owner\":\n",
    "                    details[\"owner\"] = data_type\n",
    "                elif col_name == \"Comment\":\n",
    "                    details[\"comment\"] = data_type\n",
    "                elif col_name == \"Table Properties\":\n",
    "                    for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                        if \"=\" in prop:\n",
    "                            k, v = prop.split(\"=\", 1)\n",
    "                            table_properties[k.strip()] = v.strip()\n",
    "        details[\"table_properties\"] = table_properties\n",
    "        return details\n",
    "\n",
    "    def _extract_constraints(self, describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "        constraints = []\n",
    "        in_constraints = False\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "            if col_name == \"# Constraints\":\n",
    "                in_constraints = True\n",
    "                continue\n",
    "            if in_constraints:\n",
    "                if not col_name or col_name.startswith(\"#\"):\n",
    "                    break\n",
    "                if col_name and data_type:\n",
    "                    constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "        return constraints\n",
    "\n",
    "    def _build_columns(self, columns_raw: List[Dict[str, Any]], col_tags: Dict[str, Dict[str, Any]], col_checks: Dict[str, Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n",
    "        columns = {}\n",
    "        for idx, col in enumerate(columns_raw, start=1):\n",
    "            name = col[\"name\"]\n",
    "            columns[idx] = {\n",
    "                \"name\": name,\n",
    "                \"datatype\": col[\"datatype\"],\n",
    "                \"nullable\": None,  # Could be filled with extended logic\n",
    "                \"active\": True,\n",
    "                \"comment\": col.get(\"comment\", \"\"),\n",
    "                \"tags\": col_tags.get(name, {}),\n",
    "                \"column_masking_rule\": \"\",  # No masking in snapshot, set empty\n",
    "                \"column_check_constraints\": col_checks.get(name, {}),\n",
    "            }\n",
    "        return columns\n",
    "    \n",
    "    def _get_foreign_keys(self, uc_metadata: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def _get_unique_keys(self, uc_metadata: Dict[str, List[Dict[str, Any]]]) -> List[List[str]]:\n",
    "        return []\n",
    "\n",
    "    def build_table_metadata_dict(self) -> Dict[str, Any]:\n",
    "        uc_metadata = self._get_metadata_snapshot()\n",
    "        describe_rows = self._get_describe_rows()\n",
    "\n",
    "        catalog, schema, table = self.catalog, self.schema, self.table\n",
    "\n",
    "        table_tags = {row[\"tag_name\"]: row[\"tag_value\"] for row in uc_metadata.get(\"table_tags\", [])}\n",
    "        details = self._extract_table_details(describe_rows)\n",
    "        table_check_constraints = {\n",
    "            k: {\"name\": k, \"expression\": v}\n",
    "            for k, v in details.get(\"table_properties\", {}).items()\n",
    "            if k.startswith(\"delta.constraints\")\n",
    "        }\n",
    "\n",
    "        row_filters = {}\n",
    "        for row in uc_metadata.get(\"row_filters\", []):\n",
    "            fname = row.get(\"filter_name\")\n",
    "            if fname:\n",
    "                row_filters[fname] = {\n",
    "                    \"name\": fname,\n",
    "                    \"expression\": row.get(\"target_columns\", \"\")\n",
    "                }\n",
    "\n",
    "        # Partition columns (fixed logic)\n",
    "        partitioned_by = self._extract_partitioned_by(describe_rows)\n",
    "\n",
    "        constraints = self._extract_constraints(describe_rows)\n",
    "        pk = []\n",
    "        for c in constraints:\n",
    "            if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "                m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "                if m:\n",
    "                    pk = [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "\n",
    "        columns_raw = self._extract_columns(describe_rows)\n",
    "        col_tag_lookup = {}\n",
    "        for row in uc_metadata.get(\"column_tags\", []):\n",
    "            col = row[\"column_name\"]\n",
    "            if col not in col_tag_lookup:\n",
    "                col_tag_lookup[col] = {}\n",
    "            col_tag_lookup[col][row[\"tag_name\"]] = row[\"tag_value\"]\n",
    "\n",
    "        col_constraint_lookup = {}\n",
    "        for row in uc_metadata.get(\"constraint_column_usage\", []):\n",
    "            col = row[\"column_name\"]\n",
    "            cons = row[\"constraint_name\"]\n",
    "            if col not in col_constraint_lookup:\n",
    "                col_constraint_lookup[col] = {}\n",
    "            col_constraint_lookup[col][cons] = {\"name\": cons}  # no expression parsing here\n",
    "\n",
    "        columns = self._build_columns(columns_raw, col_tag_lookup, col_constraint_lookup)\n",
    "\n",
    "        return {\n",
    "            \"full_table_name\": self.fq_table,\n",
    "            \"catalog\": catalog,\n",
    "            \"schema\": schema,\n",
    "            \"table\": table,\n",
    "            \"primary_key\": pk,\n",
    "            \"foreign_keys\": self._get_foreign_keys(uc_metadata),\n",
    "            \"unique_keys\": self._get_unique_keys(uc_metadata),\n",
    "            \"partitioned_by\": partitioned_by,\n",
    "            \"tags\": table_tags,\n",
    "            \"row_filters\": row_filters,\n",
    "            \"table_check_constraints\": table_check_constraints,\n",
    "            \"table_properties\": details.get(\"table_properties\", {}),\n",
    "            \"comment\": details.get(\"comment\", \"\"),\n",
    "            \"owner\": details.get(\"owner\", \"\"),\n",
    "            \"columns\": columns,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef7474e-ea47-463b-9203-87a99795ed61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "metadata_dict = TableSnapshot(spark, \"dq_dev.lmg_sandbox.config_driven_table_example\")\n",
    "table_snapshot = metadata_dict.build_table_metadata_dict()\n",
    "print(table_snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425d32e4-e16f-4fc3-b69d-ecff676ae886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(table_snapshot, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1a5465-81e6-4795-ab8c-7bd766bfaec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffdef1a1-4dff-4c17-b696-dd71495f2915",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "snapshot_yaml.py"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class YamlSnapshot:\n",
    "    \"\"\"\n",
    "    Loader for YAML DDL config files. Exposes all config blocks with a clean API.\n",
    "    Handles dynamic env, catalog suffixes, and nested constraints/keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str, env: Optional[str] = None):\n",
    "        self.config_path = config_path\n",
    "        self._env = env\n",
    "        self._config: Dict[str, Any] = {}\n",
    "        self.load_config()\n",
    "\n",
    "    def load_config(self) -> None:\n",
    "        try:\n",
    "            with open(self.config_path, \"r\") as f:\n",
    "                self._config = yaml.safe_load(f)\n",
    "        except (FileNotFoundError, yaml.YAMLError) as e:\n",
    "            raise ValueError(f\"Error loading YAML configuration from {self.config_path}: {e}\")\n",
    "\n",
    "    @property\n",
    "    def catalog(self) -> str:\n",
    "        return self._config.get(\"catalog\", \"\")\n",
    "\n",
    "    @property\n",
    "    def schema(self) -> str:\n",
    "        return self._config.get(\"schema\", \"\")\n",
    "\n",
    "    @property\n",
    "    def table(self) -> str:\n",
    "        return self._config.get(\"table\", \"\")\n",
    "\n",
    "    @property\n",
    "    def env(self) -> Optional[str]:\n",
    "        return self._env\n",
    "\n",
    "    @property\n",
    "    def full_table_name(self) -> str:\n",
    "        cat = self.catalog.strip()\n",
    "        sch = self.schema.strip()\n",
    "        tbl = self.table.strip()\n",
    "        env = self.env\n",
    "        if cat.endswith(\"_\") and env:\n",
    "            cat_full = f\"{cat}{env}\"\n",
    "        else:\n",
    "            cat_full = cat\n",
    "        return f\"{cat_full}.{sch}.{tbl}\"\n",
    "\n",
    "    @property\n",
    "    def owner(self) -> str:\n",
    "        return self._config.get(\"owner\", \"\")\n",
    "\n",
    "    @property\n",
    "    def tags(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"tags\", {})\n",
    "\n",
    "    @property\n",
    "    def properties(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"properties\", {})\n",
    "\n",
    "    @property\n",
    "    def table_comment(self) -> str:\n",
    "        return self.properties.get(\"comment\", \"\")\n",
    "\n",
    "    @property\n",
    "    def table_properties(self) -> Dict[str, Any]:\n",
    "        return self.properties.get(\"table_properties\", {})\n",
    "\n",
    "    @property\n",
    "    def primary_key(self) -> List[str]:\n",
    "        pk = self._config.get(\"primary_key\", [])\n",
    "        return pk if isinstance(pk, list) else [pk]\n",
    "\n",
    "    @property\n",
    "    def partitioned_by(self) -> List[str]:\n",
    "        pb = self._config.get(\"partitioned_by\", [])\n",
    "        return pb if isinstance(pb, list) else [pb]\n",
    "\n",
    "    @property\n",
    "    def unique_keys(self) -> List[List[str]]:\n",
    "        return self._config.get(\"unique_keys\", [])\n",
    "\n",
    "    @property\n",
    "    def foreign_keys(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"foreign_keys\", {})\n",
    "\n",
    "    @property\n",
    "    def table_check_constraints(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"table_check_constraints\", {})\n",
    "\n",
    "    @property\n",
    "    def row_filters(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"row_filters\", {})\n",
    "\n",
    "    @property\n",
    "    def columns(self) -> List[Dict[str, Any]]:\n",
    "        cols_dict = self._config.get(\"columns\", {})\n",
    "        cols_dict_str = {str(k): v for k, v in cols_dict.items()}\n",
    "        sorted_keys = sorted(map(int, cols_dict_str.keys()))\n",
    "        return [cols_dict_str[str(k)] for k in sorted_keys]\n",
    "\n",
    "    def build_table_metadata_dict(self) -> Dict[str, Any]:\n",
    "        # Return dict with keys in the exact order you want — relies on Python 3.7+ insertion order preservation\n",
    "        result = {\n",
    "            \"full_table_name\": self.full_table_name,\n",
    "            \"catalog\": self.catalog,\n",
    "            \"schema\": self.schema,\n",
    "            \"table\": self.table,\n",
    "            \"primary_key\": self.primary_key if self.primary_key else [],\n",
    "            \"foreign_keys\": self.foreign_keys if self.foreign_keys else {},\n",
    "            \"unique_keys\": self.unique_keys if self.unique_keys else [],\n",
    "            \"partitioned_by\": self.partitioned_by if self.partitioned_by else [],\n",
    "            \"tags\": self.tags if self.tags else {},\n",
    "            \"row_filters\": self.row_filters if self.row_filters else {},\n",
    "            \"table_check_constraints\": self.table_check_constraints if self.table_check_constraints else {},\n",
    "            \"table_properties\": self.table_properties if self.table_properties else {},\n",
    "            \"comment\": self.table_comment,\n",
    "            \"owner\": self.owner,\n",
    "            \"columns\": {},\n",
    "        }\n",
    "\n",
    "        # Numbered columns with requested keys & order\n",
    "        for idx, col in enumerate(self.columns, 1):\n",
    "            result[\"columns\"][idx] = {\n",
    "                \"name\": col.get(\"name\", \"\"),\n",
    "                \"datatype\": col.get(\"datatype\", \"\"),\n",
    "                \"nullable\": col.get(\"nullable\", True),\n",
    "                \"active\": col.get(\"active\", True),\n",
    "                \"comment\": col.get(\"comment\", \"\"),\n",
    "                \"tags\": col.get(\"tags\", {}),\n",
    "                \"column_masking_rule\": col.get(\"column_masking_rule\", \"\"),\n",
    "                \"column_check_constraints\": col.get(\"column_check_constraints\", {}),\n",
    "            }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def describe(self) -> None:\n",
    "        # Helper for dev/test use only\n",
    "        print(f\"Table: {self.full_table_name}\")\n",
    "        print(f\"  Owner: {self.owner}\")\n",
    "        print(f\"  Tags: {self.tags}\")\n",
    "        print(f\"  Primary Key: {self.primary_key}\")\n",
    "        print(f\"  Partitioned By: {self.partitioned_by}\")\n",
    "        print(f\"  Unique Keys: {self.unique_keys}\")\n",
    "        print(f\"  Foreign Keys: {self.foreign_keys}\")\n",
    "        print(f\"  Table Check Constraints: {self.table_check_constraints}\")\n",
    "        print(f\"  Row Filters: {self.row_filters}\")\n",
    "        print(f\"  Table Properties: {self.table_properties}\")\n",
    "        print(f\"  Columns:\")\n",
    "        for i, col in enumerate(self.columns, 1):\n",
    "            print(\n",
    "                f\"    {i}: {col.get('name','')} ({col.get('datatype','')}, nullable={col.get('nullable', True)}) | \"\n",
    "                f\"comment={col.get('comment','')}, tags={col.get('tags',{})}, active={col.get('active', True)}\"\n",
    "            )\n",
    "            ccc = col.get(\"column_check_constraints\", {})\n",
    "            if ccc:\n",
    "                print(f\"      Column Check Constraints: {ccc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d73088-1cef-4fd6-b016-5bd0f6e822a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yaml_path = \"layker/resources/example.yaml\"\n",
    "\n",
    "cfg = YamlSnapshot(yaml_path, env=\"dev\")\n",
    "yaml_snapshot = cfg.build_table_metadata_dict()\n",
    "print(yaml_snapshot)\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(table_meta, width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660c5dac-a429-4bad-a11a-ad44f80f9d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(yaml_snapshot, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c36a67-31c9-4b0a-adac-28b7b99f311f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5028f3-bbee-4a40-b755-e3e09dbbfd12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "differences.py"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def diff_primary_key(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add, update):\n",
    "    y_pk = yaml.get(\"primary_key\", [])\n",
    "    t_pk = table.get(\"primary_key\", []) if table else []\n",
    "    if y_pk and y_pk != t_pk:\n",
    "        if not t_pk:\n",
    "            add[\"primary_key\"] = y_pk\n",
    "        else:\n",
    "            update[\"primary_key\"] = y_pk\n",
    "\n",
    "def diff_partitioned_by(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add):\n",
    "    y_pb = yaml.get(\"partitioned_by\", [])\n",
    "    t_pb = table.get(\"partitioned_by\", []) if table else []\n",
    "    if y_pb and not t_pb:\n",
    "        add[\"partitioned_by\"] = y_pb\n",
    "\n",
    "def diff_unique_keys(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add):\n",
    "    y_uk = yaml.get(\"unique_keys\", [])\n",
    "    t_uk = table.get(\"unique_keys\", []) if table else []\n",
    "    if y_uk and y_uk != t_uk:\n",
    "        add[\"unique_keys\"] = y_uk\n",
    "\n",
    "def diff_foreign_keys(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add):\n",
    "    y_fk = yaml.get(\"foreign_keys\", {})\n",
    "    t_fk = table.get(\"foreign_keys\", {}) if table else {}\n",
    "    if y_fk and y_fk != t_fk:\n",
    "        add[\"foreign_keys\"] = y_fk\n",
    "\n",
    "def diff_table_check_constraints(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add, update, remove):\n",
    "    y_tcc = yaml.get(\"table_check_constraints\", {})\n",
    "    t_tcc = table.get(\"table_check_constraints\", {}) if table else {}\n",
    "    for k, v in y_tcc.items():\n",
    "        if k not in t_tcc:\n",
    "            add[\"table_check_constraints\"][k] = v\n",
    "        elif t_tcc[k] != v:\n",
    "            update[\"table_check_constraints\"][k] = v\n",
    "    for k, v in t_tcc.items():\n",
    "        if k not in y_tcc:\n",
    "            remove[\"table_check_constraints\"][k] = v\n",
    "\n",
    "def diff_row_filters(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add, update, remove):\n",
    "    y_rf = yaml.get(\"row_filters\", {})\n",
    "    t_rf = table.get(\"row_filters\", {}) if table else {}\n",
    "    for k, v in y_rf.items():\n",
    "        if k not in t_rf:\n",
    "            add[\"row_filters\"][k] = v\n",
    "        elif t_rf[k] != v:\n",
    "            update[\"row_filters\"][k] = v\n",
    "    for k, v in t_rf.items():\n",
    "        if k not in y_rf:\n",
    "            remove[\"row_filters\"][k] = v\n",
    "\n",
    "def diff_table_tags(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add, remove):\n",
    "    y_tags = yaml.get(\"tags\", {})\n",
    "    t_tags = table.get(\"tags\", {}) if table else {}\n",
    "    for k, v in y_tags.items():\n",
    "        if k not in t_tags:\n",
    "            add[\"table_tags\"][k] = v\n",
    "    for k, v in t_tags.items():\n",
    "        if k not in y_tags:\n",
    "            remove[\"table_tags\"][k] = v\n",
    "\n",
    "def diff_owner(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], update):\n",
    "    y_owner = yaml.get(\"owner\", \"\")\n",
    "    t_owner = table.get(\"owner\", \"\") if table else \"\"\n",
    "    if y_owner and y_owner != t_owner:\n",
    "        update[\"owner\"] = y_owner\n",
    "\n",
    "def diff_table_comment(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], update):\n",
    "    y_comment = (yaml.get(\"comment\", \"\") or \"\").strip()\n",
    "    t_comment = (table.get(\"comment\", \"\") or \"\").strip() if table else \"\"\n",
    "    if y_comment and y_comment != t_comment:\n",
    "        update[\"table_comment\"] = y_comment\n",
    "\n",
    "def diff_table_properties(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add):\n",
    "    y_props = yaml.get(\"table_properties\", {})\n",
    "    t_props = table.get(\"table_properties\", {}) if table else {}\n",
    "    for k, v in y_props.items():\n",
    "        if k not in t_props:\n",
    "            add[\"table_properties\"][k] = v\n",
    "\n",
    "def diff_columns(yaml: Dict[str, Any], table: Optional[Dict[str, Any]], add, update, remove):\n",
    "    y_cols = yaml.get(\"columns\", {}) or {}\n",
    "    t_cols = table.get(\"columns\", {}) if table else {}\n",
    "    y_idxs, t_idxs = set(y_cols.keys()), set(t_cols.keys())\n",
    "\n",
    "    # Add new columns at the end\n",
    "    max_t_idx = max(t_idxs) if t_idxs else 0\n",
    "    for idx in y_idxs:\n",
    "        if idx > max_t_idx:\n",
    "            y_col = y_cols[idx]\n",
    "            add[\"columns\"][idx] = {\n",
    "                \"name\": y_col.get(\"name\", \"\"),\n",
    "                \"datatype\": y_col.get(\"datatype\", \"\"),\n",
    "                \"nullable\": y_col.get(\"nullable\", True),\n",
    "                \"column_comment\": y_col.get(\"comment\", \"\"),\n",
    "                \"column_tags\": y_col.get(\"tags\", {}),\n",
    "                \"column_masking_rule\": y_col.get(\"column_masking_rule\", \"\"),\n",
    "                \"column_check_constraints\": y_col.get(\"column_check_constraints\", {}),\n",
    "            }\n",
    "    # Remove columns missing in YAML\n",
    "    for idx in t_idxs:\n",
    "        if idx not in y_idxs:\n",
    "            t_col = t_cols[idx]\n",
    "            remove[\"columns\"][idx] = {\n",
    "                \"name\": t_col.get(\"name\", \"\"),\n",
    "                \"column_tags\": t_col.get(\"tags\", {}),\n",
    "                \"column_check_constraints\": t_col.get(\"column_check_constraints\", {})\n",
    "            }\n",
    "    # Per-column tag/check constraint add/remove\n",
    "    for idx in y_idxs & t_idxs:\n",
    "        y_col, t_col = y_cols[idx], t_cols[idx]\n",
    "        col_update = {}\n",
    "\n",
    "        # Name change = rename (update)\n",
    "        if y_col.get(\"name\", \"\") != t_col.get(\"name\", \"\"):\n",
    "            col_update[\"name\"] = y_col.get(\"name\", \"\")\n",
    "        # Comment update\n",
    "        if y_col.get(\"comment\", \"\") != t_col.get(\"comment\", \"\"):\n",
    "            col_update[\"column_comment\"] = y_col.get(\"comment\", \"\")\n",
    "        # Masking rule update\n",
    "        if y_col.get(\"column_masking_rule\", \"\") != t_col.get(\"column_masking_rule\", \"\"):\n",
    "            col_update[\"column_masking_rule\"] = y_col.get(\"column_masking_rule\", \"\")\n",
    "\n",
    "        # --- Column tags: only add new, remove missing ---\n",
    "        y_ctags, t_ctags = y_col.get(\"tags\", {}) or {}, t_col.get(\"tags\", {}) or {}\n",
    "        tag_add, tag_remove = {}, {}\n",
    "        for k, v in y_ctags.items():\n",
    "            if k not in t_ctags:\n",
    "                tag_add[k] = v\n",
    "        for k, v in t_ctags.items():\n",
    "            if k not in y_ctags:\n",
    "                tag_remove[k] = v\n",
    "        if tag_add:\n",
    "            col_update[\"column_tags\"] = tag_add\n",
    "        if tag_remove:\n",
    "            remove[\"columns\"].setdefault(idx, {}).setdefault(\"column_tags\", {}).update(tag_remove)\n",
    "\n",
    "        # --- Column check constraints: add new, remove missing ---\n",
    "        y_cc, t_cc = y_col.get(\"column_check_constraints\", {}) or {}, t_col.get(\"column_check_constraints\", {}) or {}\n",
    "        cc_add, cc_remove = {}, {}\n",
    "        for k, v in y_cc.items():\n",
    "            if k not in t_cc:\n",
    "                cc_add[k] = v\n",
    "        for k, v in t_cc.items():\n",
    "            if k not in y_cc:\n",
    "                cc_remove[k] = v\n",
    "        if cc_add:\n",
    "            col_update[\"column_check_constraints\"] = cc_add\n",
    "        if cc_remove:\n",
    "            remove[\"columns\"].setdefault(idx, {}).setdefault(\"column_check_constraints\", {}).update(cc_remove)\n",
    "\n",
    "        if col_update:\n",
    "            update[\"columns\"][idx] = col_update\n",
    "\n",
    "def generate_differences(\n",
    "    yaml_snapshot: Dict[str, Any],\n",
    "    table_snapshot: Optional[Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute differences between YAML snapshot and table snapshot, enforcing Layker semantics.\n",
    "    Handles full create (table_snapshot is None) and incremental changes.\n",
    "    \"\"\"\n",
    "    # FULL CREATE: If table does not exist, return all YAML fields under 'add'\n",
    "    if table_snapshot is None:\n",
    "        # You could filter out empty fields if you want, but generally you want everything needed to create.\n",
    "        return {\n",
    "            \"full_table_name\": yaml_snapshot.get(\"full_table_name\", \"\"),\n",
    "            \"add\": {k: v for k, v in yaml_snapshot.items() if k != \"full_table_name\"}\n",
    "        }\n",
    "\n",
    "    diffs = {\n",
    "        \"full_table_name\": yaml_snapshot.get(\"full_table_name\", \"\"),\n",
    "        \"add\": {\n",
    "            \"primary_key\": [],\n",
    "            \"partitioned_by\": [],\n",
    "            \"unique_keys\": [],\n",
    "            \"foreign_keys\": {},\n",
    "            \"table_check_constraints\": {},\n",
    "            \"row_filters\": {},\n",
    "            \"table_tags\": {},\n",
    "            \"owner\": \"\",\n",
    "            \"table_comment\": \"\",\n",
    "            \"table_properties\": {},\n",
    "            \"columns\": {},\n",
    "        },\n",
    "        \"update\": {\n",
    "            \"primary_key\": [],\n",
    "            \"table_check_constraints\": {},\n",
    "            \"row_filters\": {},\n",
    "            \"table_tags\": {},\n",
    "            \"owner\": \"\",\n",
    "            \"table_comment\": \"\",\n",
    "            \"columns\": {},\n",
    "        },\n",
    "        \"remove\": {\n",
    "            \"table_check_constraints\": {},\n",
    "            \"row_filters\": {},\n",
    "            \"table_tags\": {},\n",
    "            \"columns\": {},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Dispatch to rules\n",
    "    diff_primary_key(yaml_snapshot, table_snapshot, diffs[\"add\"], diffs[\"update\"])\n",
    "    diff_partitioned_by(yaml_snapshot, table_snapshot, diffs[\"add\"])\n",
    "    diff_unique_keys(yaml_snapshot, table_snapshot, diffs[\"add\"])\n",
    "    diff_foreign_keys(yaml_snapshot, table_snapshot, diffs[\"add\"])\n",
    "    diff_table_check_constraints(yaml_snapshot, table_snapshot, diffs[\"add\"], diffs[\"update\"], diffs[\"remove\"])\n",
    "    diff_row_filters(yaml_snapshot, table_snapshot, diffs[\"add\"], diffs[\"update\"], diffs[\"remove\"])\n",
    "    diff_table_tags(yaml_snapshot, table_snapshot, diffs[\"add\"], diffs[\"remove\"])\n",
    "    diff_owner(yaml_snapshot, table_snapshot, diffs[\"update\"])\n",
    "    diff_table_comment(yaml_snapshot, table_snapshot, diffs[\"update\"])\n",
    "    diff_table_properties(yaml_snapshot, table_snapshot, diffs[\"add\"])\n",
    "    diff_columns(yaml_snapshot, table_snapshot, diffs[\"add\"], diffs[\"update\"], diffs[\"remove\"])\n",
    "\n",
    "    # Clean up: only return keys that have values (don't return empty add/update/remove sections)\n",
    "    out = {\"full_table_name\": diffs[\"full_table_name\"]}\n",
    "    for section in [\"add\", \"update\", \"remove\"]:\n",
    "        filtered = {k: v for k, v in diffs[section].items() if v and (not isinstance(v, dict) or len(v))}\n",
    "        if filtered:\n",
    "            out[section] = filtered\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbe215d-821d-4c0c-9f61-ca6876438e62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run -> differences.py"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from layker.snapshot_yaml import validate_and_snapshot_yaml\n",
    "#from layker.snapshot_table import TableSnapshot\n",
    "\n",
    "yaml_path = \"layker/resources/test.yaml\"\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "snapshot_yaml, fq_table = validate_and_snapshot_yaml(yaml_path, env=None, mode=\"all\")\n",
    "#table_snapshot = TableSnapshot(spark, table_name).build_table_metadata_dict()\n",
    "\n",
    "diffs = generate_differences(snapshot_yaml, None)\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b24bcc7-31cf-48d7-9f53-809a769bf67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(diffs, width=120, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "690772b3-5d94-41c6-9e26-7b9513af0768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a782c0-a61e-4e2f-8f83-bc7446bfc8e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "loader.py"
    }
   },
   "outputs": [],
   "source": [
    "# src/layker/loader.py\n",
    "\n",
    "from typing import Any, Dict\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ---- Centralized Loader Config ----\n",
    "LOADER_CONFIG = {\n",
    "    \"add\": {\n",
    "        \"primary_key\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ADD PRIMARY KEY ({cols})\",\n",
    "            \"desc\": \"ADD primary key: {cols}\"\n",
    "        },\n",
    "        \"partitioned_by\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ADD PARTITIONED BY ({cols})\",\n",
    "            \"desc\": \"ADD partition: {cols}\"\n",
    "        },\n",
    "        \"unique_keys\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ADD CONSTRAINT uq_{key} UNIQUE ({cols})\",\n",
    "            \"desc\": \"ADD unique key: {cols}\"\n",
    "        },\n",
    "        \"foreign_keys\": {\n",
    "            \"sql\": (\n",
    "                \"ALTER TABLE {fq} ADD CONSTRAINT {name} \"\n",
    "                \"FOREIGN KEY ({cols}) REFERENCES {ref_tbl} ({ref_cols})\"\n",
    "            ),\n",
    "            \"desc\": \"ADD foreign key: {name} ({cols})\"\n",
    "        },\n",
    "        \"table_check_constraints\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ADD CONSTRAINT {name} CHECK ({expression})\",\n",
    "            \"desc\": \"ADD table check constraint: {name}\"\n",
    "        },\n",
    "        \"row_filters\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ADD ROW FILTER {name} WHERE {expression}\",\n",
    "            \"desc\": \"ADD row filter: {name}\"\n",
    "        },\n",
    "        \"table_tags\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} SET TAGS ('{key}' = '{val}')\",\n",
    "            \"desc\": \"ADD table tag: {key}={val}\"\n",
    "        },\n",
    "        \"owner\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} OWNER TO `{owner}`\",\n",
    "            \"desc\": \"SET owner: {owner}\"\n",
    "        },\n",
    "        \"table_comment\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} SET COMMENT '{comment}'\",\n",
    "            \"desc\": \"SET table comment\"\n",
    "        },\n",
    "        \"table_properties\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} SET TBLPROPERTIES ('{key}' = '{val}')\",\n",
    "            \"desc\": \"ADD table property: {key}={val}\"\n",
    "        },\n",
    "        \"columns\": {\n",
    "            \"sql\": None  # Handled separately or via create\n",
    "        }\n",
    "    },\n",
    "    \"update\": {\n",
    "        \"primary_key\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ALTER PRIMARY KEY ({cols})\",\n",
    "            \"desc\": \"UPDATE primary key: {cols}\"\n",
    "        },\n",
    "        \"table_check_constraints\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ALTER CONSTRAINT {name} CHECK ({expression})\",\n",
    "            \"desc\": \"UPDATE table check constraint: {name}\"\n",
    "        },\n",
    "        \"row_filters\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} ALTER ROW FILTER {name} WHERE {expression}\",\n",
    "            \"desc\": \"UPDATE row filter: {name}\"\n",
    "        },\n",
    "        \"table_tags\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} SET TAGS ('{key}' = '{val}')\",\n",
    "            \"desc\": \"UPDATE table tag: {key}={val}\"\n",
    "        },\n",
    "        \"owner\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} OWNER TO `{owner}`\",\n",
    "            \"desc\": \"UPDATE owner: {owner}\"\n",
    "        },\n",
    "        \"table_comment\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} SET COMMENT '{comment}'\",\n",
    "            \"desc\": \"UPDATE table comment\"\n",
    "        },\n",
    "        \"table_properties\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} SET TBLPROPERTIES ('{key}' = '{val}')\",\n",
    "            \"desc\": \"UPDATE table property: {key}={val}\"\n",
    "        },\n",
    "        \"columns\": {\n",
    "            \"sql\": None  # Handled separately\n",
    "        }\n",
    "    },\n",
    "    \"remove\": {\n",
    "        \"table_check_constraints\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} DROP CONSTRAINT {name}\",\n",
    "            \"desc\": \"REMOVE table check constraint: {name}\"\n",
    "        },\n",
    "        \"row_filters\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} DROP ROW FILTER {name}\",\n",
    "            \"desc\": \"REMOVE row filter: {name}\"\n",
    "        },\n",
    "        \"table_tags\": {\n",
    "            \"sql\": \"ALTER TABLE {fq} UNSET TAGS ('{key}')\",\n",
    "            \"desc\": \"REMOVE table tag: {key}\"\n",
    "        },\n",
    "        \"columns\": {\n",
    "            \"sql\": None  # Handled separately\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class DatabricksTableLoader:\n",
    "    \"\"\"\n",
    "    Loads, updates, or removes table metadata using a differences dictionary.\n",
    "    Handles CREATE TABLE if the add block implies creation.\n",
    "    Applies column comments and tags after CREATE for all columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, diff_dict: Dict[str, Any], spark: SparkSession, dry_run: bool = False):\n",
    "        self.diff = diff_dict\n",
    "        self.spark = spark\n",
    "        self.dry_run = dry_run\n",
    "        self.fq = diff_dict[\"full_table_name\"]\n",
    "        self.log = []\n",
    "\n",
    "    def run(self):\n",
    "        add = self.diff.get(\"add\", {})\n",
    "        update = self.diff.get(\"update\", {})\n",
    "        remove = self.diff.get(\"remove\", {})\n",
    "        columns = add.get(\"columns\", {})\n",
    "\n",
    "        # --- CREATE TABLE path ---\n",
    "        if columns and str(min(map(int, columns.keys()))) == \"1\" and not update and not remove:\n",
    "            self._create_table(add)\n",
    "            print(\"[SUMMARY] Table CREATE complete:\")\n",
    "            for entry in self.log:\n",
    "                print(f\"  - {entry}\")\n",
    "            return\n",
    "\n",
    "        # --- ALTER TABLE path ---\n",
    "        for action in [\"add\", \"update\", \"remove\"]:\n",
    "            section = self.diff.get(action, {})\n",
    "            if section:\n",
    "                self._handle_section(action, section)\n",
    "        print(\"[SUMMARY] Table modifications complete:\")\n",
    "        for entry in self.log:\n",
    "            print(f\"  - {entry}\")\n",
    "\n",
    "    def _create_table(self, add_section):\n",
    "        cols = add_section[\"columns\"]\n",
    "        col_sqls = []\n",
    "        for idx in sorted(cols, key=lambda x: int(x)):\n",
    "            col = cols[idx]\n",
    "            name = col[\"name\"]\n",
    "            datatype = col[\"datatype\"]\n",
    "            nullable = col.get(\"nullable\", True)\n",
    "            col_sql = f\"`{name}` {datatype}{' NOT NULL' if not nullable else ''}\"\n",
    "            col_sqls.append(col_sql)\n",
    "        columns_sql = \",\\n  \".join(col_sqls)\n",
    "\n",
    "        # Partitioning\n",
    "        partitioned_by = add_section.get(\"partitioned_by\", [])\n",
    "        partition_sql = f\"\\nPARTITIONED BY ({', '.join(partitioned_by)})\" if partitioned_by else \"\"\n",
    "\n",
    "        # Properties\n",
    "        tbl_props = add_section.get(\"table_properties\", {})\n",
    "        tbl_props_sql = \"\"\n",
    "        if tbl_props:\n",
    "            props = [f\"'{k}' = '{v}'\" for k, v in tbl_props.items()]\n",
    "            tbl_props_sql = f\"\\nTBLPROPERTIES ({', '.join(props)})\"\n",
    "\n",
    "        # Table comment\n",
    "        tbl_comment = add_section.get(\"table_comment\", \"\")\n",
    "        tbl_comment_sql = f\"\\nCOMMENT '{tbl_comment}'\" if tbl_comment else \"\"\n",
    "\n",
    "        sql = f\"CREATE TABLE {self.fq} (\\n  {columns_sql}\\n){partition_sql}{tbl_comment_sql}{tbl_props_sql}\"\n",
    "\n",
    "        self._run(sql, \"CREATE TABLE\")\n",
    "        self.log.append(f\"CREATE TABLE with columns: {list(c['name'] for c in cols.values())}\")\n",
    "\n",
    "        # --- ENSURE column comments/tags are always applied post-create ---\n",
    "        self._handle_column_comments_and_tags(cols)\n",
    "\n",
    "        # Handle table tags, owner, PK, unique, FKs, checks, etc. as ALTER TABLE after creation.\n",
    "        self._handle_post_create(add_section)\n",
    "\n",
    "    def _handle_column_comments_and_tags(self, cols):\n",
    "        for idx in sorted(cols, key=lambda x: int(x)):\n",
    "            col = cols[idx]\n",
    "            name = col[\"name\"]\n",
    "\n",
    "            # Column comment\n",
    "            comment = col.get(\"comment\", \"\")\n",
    "            if comment:\n",
    "                sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} COMMENT '{comment}'\"\n",
    "                self._run(sql, f\"ADD comment to {name}\")\n",
    "\n",
    "            # Column tags\n",
    "            tags = col.get(\"tags\") or {}\n",
    "            for tag, value in tags.items():\n",
    "                sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} SET TAGS ('{tag}' = '{value}')\"\n",
    "                self._run(sql, f\"ADD tag {tag} to {name}\")\n",
    "\n",
    "    def _handle_post_create(self, add_section):\n",
    "        for key, val in add_section.items():\n",
    "            if key in (\"columns\", \"table_properties\", \"table_comment\", \"partitioned_by\"):\n",
    "                continue\n",
    "            meta = LOADER_CONFIG[\"add\"].get(key)\n",
    "            if not meta or not val:\n",
    "                continue\n",
    "            sql_template = meta.get(\"sql\")\n",
    "            if sql_template is None:\n",
    "                continue  # Only columns handled separately, rest should all have sql\n",
    "            if key == \"primary_key\" or key == \"partitioned_by\":\n",
    "                sql = sql_template.format(fq=self.fq, cols=\", \".join(val))\n",
    "                self._run(sql, meta[\"desc\"].format(cols=\", \".join(val)))\n",
    "            elif key == \"unique_keys\":\n",
    "                for group in val:\n",
    "                    sql = sql_template.format(fq=self.fq, key=\"_\".join(group), cols=\", \".join(group))\n",
    "                    self._run(sql, meta[\"desc\"].format(cols=\", \".join(group), key=\"_\".join(group)))\n",
    "            elif key == \"foreign_keys\":\n",
    "                for fk_name, fk in val.items():\n",
    "                    sql = sql_template.format(\n",
    "                        fq=self.fq,\n",
    "                        name=fk_name,\n",
    "                        cols=\", \".join(fk.get(\"columns\", [])),\n",
    "                        ref_tbl=fk.get(\"reference_table\", \"\"),\n",
    "                        ref_cols=\", \".join(fk.get(\"reference_columns\", [])),\n",
    "                    )\n",
    "                    self._run(sql, meta[\"desc\"].format(name=fk_name, cols=\", \".join(fk.get(\"columns\", []))))\n",
    "            elif key == \"table_check_constraints\":\n",
    "                for cname, cdict in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, name=cname, expression=cdict.get(\"expression\"))\n",
    "                    self._run(sql, meta[\"desc\"].format(name=cname))\n",
    "            elif key == \"row_filters\":\n",
    "                for fname, fdict in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, name=fname, expression=fdict.get(\"expression\"))\n",
    "                    self._run(sql, meta[\"desc\"].format(name=fname))\n",
    "            elif key == \"table_tags\":\n",
    "                for k, v in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, key=k, val=v)\n",
    "                    self._run(sql, meta[\"desc\"].format(key=k, val=v))\n",
    "            elif key == \"owner\":\n",
    "                sql = sql_template.format(fq=self.fq, owner=val)\n",
    "                self._run(sql, meta[\"desc\"].format(owner=val))\n",
    "            # No table_comment/table_properties/columns here\n",
    "\n",
    "    def _handle_section(self, action: str, section_dict: Dict[str, Any]):\n",
    "        config = LOADER_CONFIG[action]\n",
    "        for key, meta in config.items():\n",
    "            val = section_dict.get(key)\n",
    "            if not val:\n",
    "                continue\n",
    "            sql_template = meta.get(\"sql\")\n",
    "            if sql_template is None:\n",
    "                self._handle_columns(action, val)\n",
    "                continue\n",
    "            if key == \"primary_key\" or key == \"partitioned_by\":\n",
    "                sql = sql_template.format(fq=self.fq, cols=\", \".join(val))\n",
    "                self._run(sql, meta[\"desc\"].format(cols=\", \".join(val)))\n",
    "            elif key == \"unique_keys\":\n",
    "                for group in val:\n",
    "                    sql = sql_template.format(fq=self.fq, key=\"_\".join(group), cols=\", \".join(group))\n",
    "                    self._run(sql, meta[\"desc\"].format(cols=\", \".join(group), key=\"_\".join(group)))\n",
    "            elif key == \"foreign_keys\":\n",
    "                for fk_name, fk in val.items():\n",
    "                    sql = sql_template.format(\n",
    "                        fq=self.fq,\n",
    "                        name=fk_name,\n",
    "                        cols=\", \".join(fk.get(\"columns\", [])),\n",
    "                        ref_tbl=fk.get(\"reference_table\", \"\"),\n",
    "                        ref_cols=\", \".join(fk.get(\"reference_columns\", [])),\n",
    "                    )\n",
    "                    self._run(sql, meta[\"desc\"].format(name=fk_name, cols=\", \".join(fk.get(\"columns\", []))))\n",
    "            elif key == \"table_check_constraints\":\n",
    "                for cname, cdict in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, name=cname, expression=cdict.get(\"expression\"))\n",
    "                    self._run(sql, meta[\"desc\"].format(name=cname))\n",
    "            elif key == \"row_filters\":\n",
    "                for fname, fdict in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, name=fname, expression=fdict.get(\"expression\"))\n",
    "                    self._run(sql, meta[\"desc\"].format(name=fname))\n",
    "            elif key == \"table_tags\":\n",
    "                for k, v in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, key=k, val=v)\n",
    "                    self._run(sql, meta[\"desc\"].format(key=k, val=v))\n",
    "            elif key == \"table_properties\":\n",
    "                for k, v in val.items():\n",
    "                    sql = sql_template.format(fq=self.fq, key=k, val=v)\n",
    "                    self._run(sql, meta[\"desc\"].format(key=k, val=v))\n",
    "            elif key == \"owner\":\n",
    "                sql = sql_template.format(fq=self.fq, owner=val)\n",
    "                self._run(sql, meta[\"desc\"].format(owner=val))\n",
    "            elif key == \"table_comment\":\n",
    "                sql = sql_template.format(fq=self.fq, comment=val)\n",
    "                self._run(sql, meta[\"desc\"])\n",
    "            else:\n",
    "                sql = sql_template.format(fq=self.fq, val=val)\n",
    "                self._run(sql, f\"{action.upper()} {key}: {val}\")\n",
    "\n",
    "    def _handle_columns(self, action: str, columns: Dict[int, Dict[str, Any]]):\n",
    "        if action == \"add\":\n",
    "            for idx, col in columns.items():\n",
    "                name = col.get(\"name\")\n",
    "                datatype = col.get(\"datatype\")\n",
    "                if not name or not datatype:\n",
    "                    continue\n",
    "                ddl = f\"`{name}` {datatype}\"\n",
    "                if not col.get(\"nullable\", True):\n",
    "                    ddl += \" NOT NULL\"\n",
    "                sql = f\"ALTER TABLE {self.fq} ADD COLUMNS ({ddl})\"\n",
    "                self._run(sql, f\"ADD column {name}\")\n",
    "                if col.get(\"comment\"):\n",
    "                    sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} COMMENT '{col['comment']}'\"\n",
    "                    self._run(sql, f\"ADD comment to {name}\")\n",
    "                for tag, value in (col.get(\"tags\") or {}).items():\n",
    "                    sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} SET TAGS ('{tag}' = '{value}')\"\n",
    "                    self._run(sql, f\"ADD tag {tag} to {name}\")\n",
    "                if col.get(\"column_masking_rule\"):\n",
    "                    self.log.append(f\"ADD masking rule for {name} (not supported)\")\n",
    "                for cc_name, cc_def in (col.get(\"column_check_constraints\") or {}).items():\n",
    "                    expr = cc_def.get(\"expression\", \"\")\n",
    "                    self.log.append(f\"ADD check constraint {cc_name} on {name}: {expr}\")\n",
    "        elif action == \"update\":\n",
    "            for idx, col in columns.items():\n",
    "                name = col.get(\"name\")\n",
    "                if not name:\n",
    "                    continue\n",
    "                if col.get(\"comment\"):\n",
    "                    sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} COMMENT '{col['comment']}'\"\n",
    "                    self._run(sql, f\"UPDATE comment for {name}\")\n",
    "                for tag, value in (col.get(\"tags\") or {}).items():\n",
    "                    sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} SET TAGS ('{tag}' = '{value}')\"\n",
    "                    self._run(sql, f\"UPDATE tag {tag} for {name}\")\n",
    "                if col.get(\"column_masking_rule\"):\n",
    "                    self.log.append(f\"UPDATE masking rule for {name} (not supported)\")\n",
    "                for cc_name, cc_def in (col.get(\"column_check_constraints\") or {}).items():\n",
    "                    expr = cc_def.get(\"expression\", \"\")\n",
    "                    self.log.append(f\"UPDATE check constraint {cc_name} on {name}: {expr}\")\n",
    "        elif action == \"remove\":\n",
    "            for idx, col in columns.items():\n",
    "                name = col.get(\"name\")\n",
    "                if name:\n",
    "                    sql = f\"ALTER TABLE {self.fq} DROP COLUMN {name}\"\n",
    "                    self._run(sql, f\"REMOVE column {name}\")\n",
    "                for tag in (col.get(\"tags\") or {}):\n",
    "                    sql = f\"ALTER TABLE {self.fq} ALTER COLUMN {name} UNSET TAGS ('{tag}')\"\n",
    "                    self._run(sql, f\"REMOVE tag {tag} from {name}\")\n",
    "                for cc_name in (col.get(\"column_check_constraints\") or {}):\n",
    "                    self.log.append(f\"REMOVE check constraint {cc_name} from {name}\")\n",
    "\n",
    "    def _run(self, sql, desc):\n",
    "        if self.dry_run:\n",
    "            print(f\"[DRY RUN] {sql}\")\n",
    "        else:\n",
    "            self.spark.sql(sql)\n",
    "        self.log.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e5e5c4-1b15-42ca-8006-4249763edd12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run -> loader.py"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from layker.snapshot_yaml import validate_and_snapshot_yaml\n",
    "from layker.snapshot_table import TableSnapshot\n",
    "from layker.differences import generate_differences\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "snapshot_yaml, fq_table = validate_and_snapshot_yaml(yaml_path:=\"layker/resources/test.yaml\", env=None, mode=\"all\")\n",
    "table_snapshot = TableSnapshot(spark, fq_table).build_table_metadata_dict()\n",
    "\n",
    "diffs = generate_differences(snapshot_yaml, table_snapshot)\n",
    "print(diffs)\n",
    "\n",
    "loader = DatabricksTableLoader(diff_dict=diffs, spark=spark, dry_run=False)\n",
    "loader.run()\n",
    "\n",
    "print(\"--- Loader Log Output ---\")\n",
    "for entry in loader.log:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e5d2136-e4c6-41c3-bfd7-3dda7eb7f752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5467650-4891-42ba-811c-913124b816b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51bc953b-0187-46ec-a109-7df795b66398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_table_load(\n",
    "    yaml_path=\"src/layker/resources/example.yaml\",\n",
    "    log_ddl=\"logs/example_comparison.yaml\",  # adjust this path if you moved logs\n",
    "    dry_run=False,\n",
    "    spark=spark,\n",
    "    env=\"prd\",\n",
    "    mode=\"all\",    # \"validate\", \"diff\", \"apply\", \"all\"\n",
    "    audit_log_table=\"src/layker/resources/audit.yaml\"  # if you want to use the built-in one\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dev_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
