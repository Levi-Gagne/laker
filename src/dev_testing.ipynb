{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fda452b-510f-40c2-9eb6-d834c3b87ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dev Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88338-81f7-46c0-a5cd-128bf3232674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "describe_extended_query = \"\"\"\n",
    "DESCRIBE EXTENDED\n",
    "  {table_name}\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(describe_extended_query.format(table_name=table_name)).show(truncate=False, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d38569-7c57-41aa-adf0-84e920dbc503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_table_show(spark, fq_table: str):\n",
    "    df = spark.sql(f\"DESCRIBE TABLE EXTENDED {fq_table}\")\n",
    "    print(\"=== Raw DataFrame Schema ===\")\n",
    "    df.printSchema()\n",
    "    print(\"=== Raw DataFrame ===\")\n",
    "    df.show(truncate=False, n=100)\n",
    "    return df\n",
    "\n",
    "def dataframe_to_rowdicts(df):\n",
    "    rows = [row.asDict() for row in df.collect()]\n",
    "    print(\"=== Collected Rows ===\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "    return rows\n",
    "\n",
    "# Get DataFrame and show it\n",
    "df = describe_table_show(spark, \"dq_dev.lmg_sandbox.config_driven_table_example\")\n",
    "\n",
    "# Convert to Python list of dicts and show those\n",
    "rows = dataframe_to_rowdicts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a36b9b6-91b3-484c-93ba-ca47f2ffaf1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_describe_table(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert the output of DESCRIBE TABLE EXTENDED (rows as dicts)\n",
    "    into a nested dict matching YAML's structure.\n",
    "    \"\"\"\n",
    "    table_level = {}\n",
    "    columns = []\n",
    "    partitioned_by = []\n",
    "    constraints = []\n",
    "    table_properties = {}\n",
    "    owner = None\n",
    "    comment = None\n",
    "\n",
    "    # State tracking\n",
    "    mode = \"columns\"\n",
    "\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comm = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "\n",
    "        # Section transitions\n",
    "        if col_name == \"# Partition Information\":\n",
    "            mode = \"partition\"\n",
    "            continue\n",
    "        elif col_name == \"# Detailed Table Information\":\n",
    "            mode = \"details\"\n",
    "            continue\n",
    "        elif col_name == \"# Constraints\":\n",
    "            mode = \"constraints\"\n",
    "            continue\n",
    "        elif col_name.startswith(\"#\"):\n",
    "            mode = \"skip\"\n",
    "            continue\n",
    "\n",
    "        if mode == \"columns\" and col_name and not col_name.startswith(\"#\"):\n",
    "            columns.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comm if comm and comm.upper() != \"NULL\" else \"\",\n",
    "                # Placeholders for additional fields\n",
    "                \"nullable\": None,\n",
    "                \"tags\": {},\n",
    "                \"column_masking_rule\": None,\n",
    "                \"default_value\": None,\n",
    "                \"variable_value\": None,\n",
    "                \"allowed_values\": [],\n",
    "                \"column_check_constraints\": {},\n",
    "                \"active\": True,\n",
    "            })\n",
    "        elif mode == \"partition\" and col_name and col_name != \"# col_name\":\n",
    "            partitioned_by.append(col_name)\n",
    "        elif mode == \"details\":\n",
    "            if col_name == \"Catalog\":\n",
    "                table_level[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                table_level[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                table_level[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                owner = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                comment = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                # Parse table properties string into dict\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "            # Add more detail parsing as needed\n",
    "\n",
    "        elif mode == \"constraints\" and col_name and data_type:\n",
    "            constraints.append((col_name, data_type))\n",
    "\n",
    "    # Compose snapshot\n",
    "    table_level[\"owner\"] = owner\n",
    "    table_level[\"comment\"] = comment\n",
    "    table_level[\"partitioned_by\"] = partitioned_by\n",
    "    table_level[\"table_properties\"] = table_properties\n",
    "    # Parse out PK/unique from constraints\n",
    "    pk = []\n",
    "    for cname, dtype in constraints:\n",
    "        if dtype.startswith(\"PRIMARY KEY\"):\n",
    "            pk.append(dtype.split(\"`\")[1].replace(\"`\", \"\"))\n",
    "    table_level[\"primary_key\"] = pk\n",
    "\n",
    "    # Final structure\n",
    "    return {\n",
    "        \"table_level_values\": table_level,\n",
    "        \"column_level_values\": columns,\n",
    "    }\n",
    "\n",
    "# ---- Example usage ----\n",
    "snapshot = parse_describe_table(rows)\n",
    "from pprint import pprint\n",
    "pprint(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af52c9a8-135f-4d78-b891-272994cdf9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def parse_fully_qualified_table(fq_table: str):\n",
    "    \"\"\"Split catalog.schema.table into (catalog, schema, table)\"\"\"\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected catalog.schema.table, got: {fq_table}\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def spark_sql_to_rows(spark: SparkSession, sql: str) -> List[dict]:\n",
    "    \"\"\"Runs a Spark SQL and returns the result as a list of dicts.\"\"\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def get_table_tags(spark: SparkSession, fq_table: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return all tags for the given table as a dict: {tag_name: tag_value, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT tag_name, tag_value\n",
    "        FROM system.information_schema.table_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    return {row['tag_name']: row['tag_value'] for row in rows}\n",
    "\n",
    "def get_column_tags(spark: SparkSession, fq_table: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Return all tags for each column in the table as:\n",
    "        {column_name: {tag_name: tag_value, ...}, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, tag_name, tag_value\n",
    "        FROM system.information_schema.column_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    col_tags = {}\n",
    "    for row in rows:\n",
    "        col = row['column_name']\n",
    "        tag = row['tag_name']\n",
    "        val = row['tag_value']\n",
    "        if col not in col_tags:\n",
    "            col_tags[col] = {}\n",
    "        col_tags[col][tag] = val\n",
    "    return col_tags\n",
    "\n",
    "def get_row_filters(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all row filters for a table as a list of dicts.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT filter_name, target_columns\n",
    "        FROM system.information_schema.row_filters\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_table_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints defined on the table (e.g. PK, Unique, FK).\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT constraint_name\n",
    "        FROM system.information_schema.constraint_table_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_column_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints for all columns on the table.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, constraint_name\n",
    "        FROM system.information_schema.constraint_column_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "  \n",
    "fq = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "  # 1. Table tags\n",
    "print(\"=== Table Tags ===\")\n",
    "print(get_table_tags(spark, fq))\n",
    "\n",
    "# 2. Column tags\n",
    "print(\"=== Column Tags ===\")\n",
    "print(get_column_tags(spark, fq))\n",
    "\n",
    "# 3. Row filters\n",
    "print(\"=== Row Filters ===\")\n",
    "print(get_row_filters(spark, fq))\n",
    "\n",
    "# 4. Table constraints\n",
    "print(\"=== Table Constraints ===\")\n",
    "print(get_constraint_table_usage(spark, fq))\n",
    "\n",
    "# 5. Column constraints\n",
    "print(\"=== Column Constraints ===\")\n",
    "print(get_constraint_column_usage(spark, fq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1797ee7c-649a-4b4e-b21e-e033a6573b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Spark SQL helpers ---\n",
    "def spark_sql_to_df(spark: SparkSession, sql: str):\n",
    "    \"\"\"\n",
    "    Run Spark SQL and return the DataFrame. Logs error, re-raises on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.sql(sql)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] spark_sql_to_df failed: {e}\\nSQL: {sql}\")\n",
    "        raise\n",
    "\n",
    "def spark_sql_to_rows(spark: SparkSession, sql: str):\n",
    "    \"\"\"\n",
    "    Run Spark SQL and return results as list of dicts. Logs error, re-raises on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.sql(sql)\n",
    "        return [row.asDict() for row in df.collect()]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] spark_sql_to_rows failed: {e}\\nSQL: {sql}\")\n",
    "        raise\n",
    "\n",
    "# --- Table introspection ---\n",
    "def describe_table_to_rows(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    sql = f\"DESCRIBE TABLE EXTENDED {fq_table}\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def extract_columns(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    cols = []\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comm = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "        if col_name and not col_name.startswith(\"#\"):\n",
    "            cols.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comm if comm and comm.upper() != \"NULL\" else \"\",\n",
    "                \"nullable\": None,\n",
    "                \"tags\": {},\n",
    "                \"column_masking_rule\": None,\n",
    "                \"default_value\": None,\n",
    "                \"variable_value\": None,\n",
    "                \"allowed_values\": [],\n",
    "                \"column_check_constraints\": {},\n",
    "                \"active\": True,\n",
    "            })\n",
    "    return cols\n",
    "\n",
    "def extract_partitioned_by(spark: SparkSession, fq_table: str) -> List[str]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        elif col_name.startswith(\"#\") and collecting:\n",
    "            break\n",
    "        elif collecting and col_name and col_name != \"# col_name\":\n",
    "            partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_details(spark: SparkSession, fq_table: str) -> Dict[str, Any]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    details = {}\n",
    "    props = {}\n",
    "    owner = None\n",
    "    comment = None\n",
    "    in_details = False\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Catalog\":\n",
    "                details[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                details[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                details[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                owner = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                comment = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        props[k.strip()] = v.strip()\n",
    "    details[\"owner\"] = owner\n",
    "    details[\"comment\"] = comment\n",
    "    details[\"table_properties\"] = props\n",
    "    return details\n",
    "\n",
    "def extract_constraints(spark: SparkSession, fq_table: str) -> List[Tuple[str, str]]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append((col_name, data_type))\n",
    "    return constraints\n",
    "\n",
    "def extract_primary_key(spark: SparkSession, fq_table: str) -> List[str]:\n",
    "    constraints = extract_constraints(spark, fq_table)\n",
    "    pk = []\n",
    "    for cname, dtype in constraints:\n",
    "        if \"PRIMARY KEY\" in dtype:\n",
    "            m = re.search(r\"\\((.*?)\\)\", dtype)\n",
    "            if m:\n",
    "                cols = [c.strip().replace(\"`\", \"\") for c in m.group(1).split(\",\")]\n",
    "                pk += cols\n",
    "    return pk\n",
    "\n",
    "def parse_describe_table(spark: SparkSession, fq_table: str) -> Dict[str, Any]:\n",
    "    details = extract_details(spark, fq_table)\n",
    "    columns = extract_columns(spark, fq_table)\n",
    "    partitioned_by = extract_partitioned_by(spark, fq_table)\n",
    "    pk = extract_primary_key(spark, fq_table)\n",
    "    details[\"partitioned_by\"] = partitioned_by\n",
    "    details[\"primary_key\"] = pk\n",
    "    return {\n",
    "        \"table_level_values\": details,\n",
    "        \"column_level_values\": columns,\n",
    "    }\n",
    "\n",
    "# ---- TEST ALL FUNCTIONS ----\n",
    "\n",
    "fq_table = \"dq_dev.lmg_sandbox.config_driven_table_example\"   # <--- update as needed\n",
    "\n",
    "print(\"==== COLUMNS ====\")\n",
    "print(extract_columns(spark, fq_table))\n",
    "\n",
    "print(\"==== PARTITIONED BY ====\")\n",
    "print(extract_partitioned_by(spark, fq_table))\n",
    "\n",
    "print(\"==== DETAILS ====\")\n",
    "print(extract_details(spark, fq_table))\n",
    "\n",
    "print(\"==== CONSTRAINTS ====\")\n",
    "print(extract_constraints(spark, fq_table))\n",
    "\n",
    "print(\"==== PRIMARY KEY ====\")\n",
    "print(extract_primary_key(spark, fq_table))\n",
    "\n",
    "print(\"==== FULL SNAPSHOT ====\")\n",
    "from pprint import pprint\n",
    "pprint(parse_describe_table(spark, fq_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cca53e-30dd-462d-bff5-714ebc28ae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_table_to_rows(spark, fq_table: str):\n",
    "    \"\"\"Run DESCRIBE TABLE EXTENDED and return rows as list of dicts.\"\"\"\n",
    "    df = spark.sql(f\"DESCRIBE TABLE EXTENDED {fq_table}\")\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "def extract_columns(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract just the column definitions from DESCRIBE output.\"\"\"\n",
    "    columns = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "\n",
    "        # Columns section (stops at # Partition Info)\n",
    "        if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "            if col_name == \"# Partition Information\":\n",
    "                break\n",
    "            continue\n",
    "        columns.append({\n",
    "            \"name\": col_name,\n",
    "            \"datatype\": data_type,\n",
    "            \"comment\": comment if comment and comment.upper() != \"NULL\" else \"\",\n",
    "            # only these three fields are real here\n",
    "        })\n",
    "    return columns\n",
    "\n",
    "def extract_partitioned_by(describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"Extract partition columns (if any).\"\"\"\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        if collecting:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name != \"# col_name\":\n",
    "                partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_table_details(describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract catalog, schema, table, owner, comment, table_properties only.\"\"\"\n",
    "    details = {}\n",
    "    table_properties = {}\n",
    "    in_details = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Catalog\":\n",
    "                details[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                details[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                details[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                details[\"owner\"] = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                details[\"comment\"] = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "    details[\"table_properties\"] = table_properties\n",
    "    return details\n",
    "\n",
    "def extract_constraints(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extract table constraints as dicts: name/type\"\"\"\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "    return constraints\n",
    "\n",
    "def extract_primary_key(describe_rows: List[Dict[str, Any]]) -> Optional[List[str]]:\n",
    "    \"\"\"Find and parse PRIMARY KEY constraint, if present.\"\"\"\n",
    "    cons = extract_constraints(describe_rows)\n",
    "    for c in cons:\n",
    "        if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "            m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "            if m:\n",
    "                return [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "    return None\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "from typing import List, Dict\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def parse_fully_qualified_table(fq_table: str):\n",
    "    \"\"\"Split catalog.schema.table into (catalog, schema, table)\"\"\"\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected catalog.schema.table, got: {fq_table}\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def spark_sql_to_rows(spark: SparkSession, sql: str) -> List[dict]:\n",
    "    \"\"\"Runs a Spark SQL and returns the result as a list of dicts.\"\"\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def get_table_tags(spark: SparkSession, fq_table: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return all tags for the given table as a dict: {tag_name: tag_value, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT tag_name, tag_value\n",
    "        FROM system.information_schema.table_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    return {row['tag_name']: row['tag_value'] for row in rows}\n",
    "\n",
    "def get_column_tags(spark: SparkSession, fq_table: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Return all tags for each column in the table as:\n",
    "        {column_name: {tag_name: tag_value, ...}, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, tag_name, tag_value\n",
    "        FROM system.information_schema.column_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    col_tags = {}\n",
    "    for row in rows:\n",
    "        col = row['column_name']\n",
    "        tag = row['tag_name']\n",
    "        val = row['tag_value']\n",
    "        if col not in col_tags:\n",
    "            col_tags[col] = {}\n",
    "        col_tags[col][tag] = val\n",
    "    return col_tags\n",
    "\n",
    "def get_row_filters(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all row filters for a table as a list of dicts.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT filter_name, target_columns\n",
    "        FROM system.information_schema.row_filters\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_table_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints defined on the table (e.g. PK, Unique, FK).\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT constraint_name\n",
    "        FROM system.information_schema.constraint_table_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_column_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints for all columns on the table.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, constraint_name\n",
    "        FROM system.information_schema.constraint_column_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a566741-5773-401d-9d47-10c62829365c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Run these in order to see exactly what you get ---\n",
    "fq = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "rows = describe_table_to_rows(spark, fq)\n",
    "\n",
    "print(\"\\n--- Columns ---\")\n",
    "print(extract_columns(rows))\n",
    "\n",
    "print(\"\\n--- Partitioned By ---\")\n",
    "print(extract_partitioned_by(rows))\n",
    "\n",
    "print(\"\\n--- Table Details ---\")\n",
    "print(extract_table_details(rows))\n",
    "\n",
    "print(\"\\n--- Constraints ---\")\n",
    "print(extract_constraints(rows))\n",
    "\n",
    "print(\"\\n--- Primary Key ---\")\n",
    "print(extract_primary_key(rows))\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# 1. Table tags\n",
    "print(\"=== Table Tags ===\")\n",
    "print(get_table_tags(spark, fq))\n",
    "\n",
    "# 2. Column tags\n",
    "print(\"=== Column Tags ===\")\n",
    "print(get_column_tags(spark, fq))\n",
    "\n",
    "# 3. Row filters\n",
    "print(\"=== Row Filters ===\")\n",
    "print(get_row_filters(spark, fq))\n",
    "\n",
    "# 4. Table constraints\n",
    "print(\"=== Table Constraints ===\")\n",
    "print(get_constraint_table_usage(spark, fq))\n",
    "\n",
    "# 5. Column constraints\n",
    "print(\"=== Column Constraints ===\")\n",
    "print(get_constraint_column_usage(spark, fq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa42ffd2-5f1f-435f-9b5e-4d67d1ee9b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Config: Slim and Correct ---\n",
    "SNAPSHOT_QUERIES = {\n",
    "    \"table_tags\": {\n",
    "        \"table\": \"system.information_schema.table_tags\",\n",
    "        \"columns\": [\"tag_name\", \"tag_value\"],\n",
    "    },\n",
    "    \"column_tags\": {\n",
    "        \"table\": \"system.information_schema.column_tags\",\n",
    "        \"columns\": [\"column_name\", \"tag_name\", \"tag_value\"],\n",
    "    },\n",
    "    \"row_filters\": {\n",
    "        \"table\": \"system.information_schema.row_filters\",\n",
    "        \"columns\": [\"filter_name\", \"target_columns\"],\n",
    "    },\n",
    "    \"constraint_table_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_table_usage\",\n",
    "        \"columns\": [\"constraint_name\"],\n",
    "    },\n",
    "    \"constraint_column_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_column_usage\",\n",
    "        \"columns\": [\"column_name\", \"constraint_name\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def parse_fully_qualified_table_name(fq_table: str):\n",
    "    \"\"\"Splits 'catalog.schema.table' into catalog, schema, table.\"\"\"\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Expected format: catalog.schema.table\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def build_metadata_sql(kind: str, fq_table: str) -> str:\n",
    "    \"\"\"Builds SQL for the given metadata kind and table.\"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table_name(fq_table)\n",
    "    config = SNAPSHOT_QUERIES[kind]\n",
    "    columns = \", \".join(config[\"columns\"])\n",
    "    return f\"\"\"\n",
    "        SELECT {columns}\n",
    "        FROM {config['table']}\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "\n",
    "# --- Table to inspect ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "# --- Run and print all queries ---\n",
    "for kind in SNAPSHOT_QUERIES:\n",
    "    print(f\"\\n--- {kind.upper()} ---\")\n",
    "    sql = build_metadata_sql(kind, table_name)\n",
    "    print(f\"SQL: {sql.strip()}\")\n",
    "    try:\n",
    "        df = spark.sql(sql)\n",
    "        rows = df.collect()\n",
    "        for row in rows:\n",
    "            print(row.asDict())\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bf02ef7-6aee-40e3-be02-0a2cf69c70c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaaf76a5-6d0c-41fa-a869-0d6ece4202e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get describe extended output as a list of dicts (describe_rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Your extract functions (copy these in above) ---\n",
    "\n",
    "def extract_columns(describe_rows):\n",
    "    columns = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "\n",
    "        if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "            if col_name == \"# Partition Information\":\n",
    "                break\n",
    "            continue\n",
    "        columns.append({\n",
    "            \"name\": col_name,\n",
    "            \"datatype\": data_type,\n",
    "            \"comment\": comment if comment and comment.upper() != \"NULL\" else \"\",\n",
    "        })\n",
    "    return columns\n",
    "\n",
    "def extract_partitioned_by(describe_rows):\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        if collecting:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name != \"# col_name\":\n",
    "                partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_table_details(describe_rows):\n",
    "    details = {}\n",
    "    table_properties = {}\n",
    "    in_details = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Owner\":\n",
    "                details[\"owner\"] = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                details[\"comment\"] = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "    details[\"table_properties\"] = table_properties\n",
    "    return details\n",
    "\n",
    "def extract_constraints(describe_rows):\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "    return constraints\n",
    "\n",
    "# --- Run each extraction and print results ---\n",
    "print(\"--- COLUMNS ---\")\n",
    "print(extract_columns(describe_rows))\n",
    "\n",
    "print(\"\\n--- PARTITIONED BY ---\")\n",
    "print(extract_partitioned_by(describe_rows))\n",
    "\n",
    "print(\"\\n--- TABLE DETAILS ---\")\n",
    "print(extract_table_details(describe_rows))\n",
    "\n",
    "print(\"\\n--- CONSTRAINTS ---\")\n",
    "print(extract_constraints(describe_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a410df1-1125-4621-990f-e4ff9bdaf23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1c91ff-94dd-4103-98db-f0ce2c58200f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- SYSTEM TABLE SNAPSHOT QUERIES ---\n",
    "SNAPSHOT_QUERIES = {\n",
    "    \"table_tags\": {\n",
    "        \"table\": \"system.information_schema.table_tags\",\n",
    "        \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"tag_name\", \"tag_value\"],\n",
    "        \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"column_tags\": {\n",
    "        \"table\": \"system.information_schema.column_tags\",\n",
    "        \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"column_name\", \"tag_name\", \"tag_value\"],\n",
    "        \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"row_filters\": {\n",
    "        \"table\": \"system.information_schema.row_filters\",\n",
    "        \"columns\": [\"table_catalog\", \"table_schema\", \"table_name\", \"filter_name\", \"target_columns\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"constraint_table_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_table_usage\",\n",
    "        \"columns\": [\"constraint_catalog\", \"constraint_schema\", \"constraint_name\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"constraint_column_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_column_usage\",\n",
    "        \"columns\": [\"column_name\", \"constraint_name\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "}\n",
    "\n",
    "def parse_fq_table(fq_table: str):\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Expected format: catalog.schema.table\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def build_metadata_sql(kind: str, fq_table: str) -> str:\n",
    "    config = SNAPSHOT_QUERIES[kind]\n",
    "    catalog, schema, table = parse_fq_table(fq_table)\n",
    "    table_vars = [catalog, schema, table]\n",
    "    where_clauses = [\n",
    "        f\"{col_name} = '{table_vars[idx]}'\"\n",
    "        for col_name, idx in config[\"where_keys\"]\n",
    "    ]\n",
    "    columns = \", \".join(config[\"columns\"])\n",
    "    return f\"SELECT {columns} FROM {config['table']} WHERE {' AND '.join(where_clauses)}\"\n",
    "\n",
    "def get_metadata_snapshot(spark: SparkSession, fq_table: str) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    results = {}\n",
    "    for kind in SNAPSHOT_QUERIES:\n",
    "        try:\n",
    "            sql = build_metadata_sql(kind, fq_table)\n",
    "            df = spark.sql(sql)\n",
    "            rows = [row.asDict() for row in df.collect()]\n",
    "            results[kind] = rows\n",
    "        except Exception as e:\n",
    "            results[kind] = f\"[ERROR] {e}\"\n",
    "    return results\n",
    "\n",
    "# --- DESCRIBE TABLE EXTENDED PARSERS ---\n",
    "def get_describe_rows(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    sql = f\"DESCRIBE EXTENDED {fq_table}\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def extract_columns(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    columns = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "        if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "            if col_name == \"# Partition Information\":\n",
    "                break\n",
    "            continue\n",
    "        columns.append({\n",
    "            \"name\": col_name,\n",
    "            \"datatype\": data_type,\n",
    "            \"comment\": comment if comment and comment.upper() != \"NULL\" else \"\",\n",
    "        })\n",
    "    return columns\n",
    "\n",
    "def extract_partitioned_by(describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        if collecting:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name != \"# col_name\":\n",
    "                partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_table_details(describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    details = {}\n",
    "    table_properties = {}\n",
    "    in_details = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Owner\":\n",
    "                details[\"owner\"] = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                details[\"comment\"] = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "    details[\"table_properties\"] = table_properties\n",
    "    return details\n",
    "\n",
    "def extract_constraints(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "    return constraints\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# --- Example usage, all prints at the bottom: ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Fetch Unity Catalog system metadata (raw output)\n",
    "uc_metadata = get_metadata_snapshot(spark, table_name)\n",
    "\n",
    "# Fetch DESCRIBE EXTENDED rows (raw output)\n",
    "describe_rows = get_describe_rows(spark, table_name)\n",
    "\n",
    "# --- Now print results ---\n",
    "for kind, rows in uc_metadata.items():\n",
    "    print(f\"\\n--- {kind.upper()} ---\")\n",
    "    if isinstance(rows, str) and rows.startswith(\"[ERROR]\"):\n",
    "        print(rows)\n",
    "    elif not rows:\n",
    "        print(\"No rows found.\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "print(\"\\n--- COLUMNS ---\")\n",
    "columns = extract_columns(describe_rows)\n",
    "print(columns if columns else \"No columns found.\")\n",
    "\n",
    "print(\"\\n--- PARTITIONED BY ---\")\n",
    "partitioned_by = extract_partitioned_by(describe_rows)\n",
    "print(partitioned_by if partitioned_by else \"No partitions found.\")\n",
    "\n",
    "print(\"\\n--- TABLE DETAILS ---\")\n",
    "table_details = extract_table_details(describe_rows)\n",
    "print(table_details if table_details else \"No table details found.\")\n",
    "\n",
    "print(\"\\n--- CONSTRAINTS ---\")\n",
    "constraints = extract_constraints(describe_rows)\n",
    "print(constraints if constraints else \"No constraints found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2648efe-25a7-46da-b666-fbcf600813cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_table_metadata_snapshot(\n",
    "    fq_table: str,\n",
    "    uc_metadata: Dict[str, List[Dict[str, Any]]],\n",
    "    describe_rows: List[Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    catalog, schema, table = parse_fq_table(fq_table)\n",
    "    # Table tags\n",
    "    table_tags = {row[\"tag_name\"]: row[\"tag_value\"] for row in uc_metadata.get(\"table_tags\", [])}\n",
    "    # Table properties, owner, comment\n",
    "    details = extract_table_details(describe_rows)\n",
    "    # Table check constraints (if present in table_properties, or elsewhere)\n",
    "    table_check_constraints = {\n",
    "        k: {\"name\": k, \"expression\": v}\n",
    "        for k, v in details.get(\"table_properties\", {}).items()\n",
    "        if k.startswith(\"delta.constraints\")\n",
    "    }\n",
    "    # Row filters\n",
    "    row_filters = [\n",
    "        {\"filter_name\": row[\"filter_name\"], \"target_columns\": row[\"target_columns\"]}\n",
    "        for row in uc_metadata.get(\"row_filters\", [])\n",
    "    ]\n",
    "    # Partition columns\n",
    "    partitioned_by = extract_partitioned_by(describe_rows)\n",
    "    # Constraints\n",
    "    constraints = extract_constraints(describe_rows)\n",
    "    # Primary key: from constraints\n",
    "    pk = []\n",
    "    for c in constraints:\n",
    "        if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "            m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "            if m:\n",
    "                pk = [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "    # Columns (by index)\n",
    "    columns_raw = extract_columns(describe_rows)\n",
    "    # Column tags (merge by column name)\n",
    "    col_tag_lookup = {}\n",
    "    for row in uc_metadata.get(\"column_tags\", []):\n",
    "        col = row[\"column_name\"]\n",
    "        if col not in col_tag_lookup:\n",
    "            col_tag_lookup[col] = {}\n",
    "        col_tag_lookup[col][row[\"tag_name\"]] = row[\"tag_value\"]\n",
    "    # Column check constraints (by constraint_column_usage)\n",
    "    col_constraint_lookup = {}\n",
    "    for row in uc_metadata.get(\"constraint_column_usage\", []):\n",
    "        col = row[\"column_name\"]\n",
    "        cons = row[\"constraint_name\"]\n",
    "        if col not in col_constraint_lookup:\n",
    "            col_constraint_lookup[col] = {}\n",
    "        col_constraint_lookup[col][cons] = {\"name\": cons}  # Expression requires deeper parsing if needed\n",
    "\n",
    "    # Build columns dictionary by position (1-based, as in your spec)\n",
    "    columns = {}\n",
    "    for idx, col in enumerate(columns_raw, start=1):\n",
    "        colname = col[\"name\"]\n",
    "        columns[idx] = {\n",
    "            \"column_name\": colname,\n",
    "            \"datatype\": col[\"datatype\"],\n",
    "            \"comment\": col[\"comment\"],\n",
    "            \"nullable\": None,  # Could be extracted if needed\n",
    "            \"masking_rule\": None,  # Could be extracted if needed\n",
    "            \"column_tags\": col_tag_lookup.get(colname, {}),\n",
    "            \"column_check_constraints\": col_constraint_lookup.get(colname, {}),\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        \"table\": {\n",
    "            \"fully_qualified_name\": fq_table,\n",
    "            \"catalog\": catalog,\n",
    "            \"schema\": schema,\n",
    "            \"table\": table,\n",
    "            \"owner\": details.get(\"owner\", \"\"),\n",
    "            \"comment\": details.get(\"comment\", \"\"),\n",
    "            \"table_properties\": details.get(\"table_properties\", {}),\n",
    "            \"table_tags\": table_tags,\n",
    "            \"table_check_constraints\": table_check_constraints,\n",
    "            \"row_filters\": row_filters,\n",
    "            \"partitioned_by\": partitioned_by,\n",
    "            \"primary_key\": pk,\n",
    "            \"columns\": columns,\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# --- Usage Example ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "uc_metadata = get_metadata_snapshot(spark, table_name)\n",
    "describe_rows = get_describe_rows(spark, table_name)\n",
    "\n",
    "snapshot = build_table_metadata_snapshot(table_name, uc_metadata, describe_rows)\n",
    "import pprint; pprint.pprint(snapshot, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b05776-ddd3-4208-8f46-62dd9801851f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157eb55e-e4ea-434a-88a8-066421ca5884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c43b123b-9155-4688-966c-1436718da3ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table Snapshot"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "############################################################\n",
    "                    Start of First Section                    \n",
    "############################################################\n",
    "\n",
    "# --- SYSTEM TABLE SNAPSHOT QUERIES ---\n",
    "SNAPSHOT_QUERIES = {\n",
    "    \"table_tags\": {\n",
    "        \"table\": \"system.information_schema.table_tags\",\n",
    "        \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"tag_name\", \"tag_value\"],\n",
    "        \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"column_tags\": {\n",
    "        \"table\": \"system.information_schema.column_tags\",\n",
    "        \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"column_name\", \"tag_name\", \"tag_value\"],\n",
    "        \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"row_filters\": {\n",
    "        \"table\": \"system.information_schema.row_filters\",\n",
    "        \"columns\": [\"table_catalog\", \"table_schema\", \"table_name\", \"filter_name\", \"target_columns\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"constraint_table_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_table_usage\",\n",
    "        \"columns\": [\"constraint_catalog\", \"constraint_schema\", \"constraint_name\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "    \"constraint_column_usage\": {\n",
    "        \"table\": \"system.information_schema.constraint_column_usage\",\n",
    "        \"columns\": [\"column_name\", \"constraint_name\"],\n",
    "        \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "    },\n",
    "}\n",
    "\n",
    "def parse_fq_table(fq_table: str):\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Expected format: catalog.schema.table\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def build_metadata_sql(kind: str, fq_table: str) -> str:\n",
    "    config = SNAPSHOT_QUERIES[kind]\n",
    "    catalog, schema, table = parse_fq_table(fq_table)\n",
    "    table_vars = [catalog, schema, table]\n",
    "    where_clauses = [\n",
    "        f\"{col_name} = '{table_vars[idx]}'\"\n",
    "        for col_name, idx in config[\"where_keys\"]\n",
    "    ]\n",
    "    columns = \", \".join(config[\"columns\"])\n",
    "    return f\"SELECT {columns} FROM {config['table']} WHERE {' AND '.join(where_clauses)}\"\n",
    "\n",
    "def get_metadata_snapshot(spark: SparkSession, fq_table: str) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    results = {}\n",
    "    for kind in SNAPSHOT_QUERIES:\n",
    "        try:\n",
    "            sql = build_metadata_sql(kind, fq_table)\n",
    "            df = spark.sql(sql)\n",
    "            rows = [row.asDict() for row in df.collect()]\n",
    "            results[kind] = rows\n",
    "        except Exception as e:\n",
    "            results[kind] = f\"[ERROR] {e}\"\n",
    "    return results\n",
    "\n",
    "# --- DESCRIBE TABLE EXTENDED PARSERS ---\n",
    "def get_describe_rows(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    sql = f\"DESCRIBE EXTENDED {fq_table}\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def extract_columns(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    columns = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "        if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "            if col_name == \"# Partition Information\":\n",
    "                break\n",
    "            continue\n",
    "        columns.append({\n",
    "            \"name\": col_name,\n",
    "            \"datatype\": data_type,\n",
    "            \"comment\": comment if comment and comment.upper() != \"NULL\" else \"\",\n",
    "        })\n",
    "    return columns\n",
    "\n",
    "def extract_partitioned_by(describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        if collecting:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name != \"# col_name\":\n",
    "                partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_table_details(describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    details = {}\n",
    "    table_properties = {}\n",
    "    in_details = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Owner\":\n",
    "                details[\"owner\"] = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                details[\"comment\"] = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "    details[\"table_properties\"] = table_properties\n",
    "    return details\n",
    "\n",
    "def extract_constraints(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "    return constraints\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "# --- Example usage, all prints at the bottom: ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Fetch Unity Catalog system metadata (raw output)\n",
    "uc_metadata = get_metadata_snapshot(spark, table_name)\n",
    "\n",
    "# Fetch DESCRIBE EXTENDED rows (raw output)\n",
    "describe_rows = get_describe_rows(spark, table_name)\n",
    "\n",
    "# --- Now print results ---\n",
    "for kind, rows in uc_metadata.items():\n",
    "    print(f\"\\n--- {kind.upper()} ---\")\n",
    "    if isinstance(rows, str) and rows.startswith(\"[ERROR]\"):\n",
    "        print(rows)\n",
    "    elif not rows:\n",
    "        print(\"No rows found.\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "print(\"\\n--- COLUMNS ---\")\n",
    "columns = extract_columns(describe_rows)\n",
    "print(columns if columns else \"No columns found.\")\n",
    "\n",
    "print(\"\\n--- PARTITIONED BY ---\")\n",
    "partitioned_by = extract_partitioned_by(describe_rows)\n",
    "print(partitioned_by if partitioned_by else \"No partitions found.\")\n",
    "\n",
    "print(\"\\n--- TABLE DETAILS ---\")\n",
    "table_details = extract_table_details(describe_rows)\n",
    "print(table_details if table_details else \"No table details found.\")\n",
    "\n",
    "print(\"\\n--- CONSTRAINTS ---\")\n",
    "constraints = extract_constraints(describe_rows)\n",
    "print(constraints if constraints else \"No constraints found.\")\n",
    "\"\"\"\n",
    "\n",
    "############################################################\n",
    "                    End of First Section                    \n",
    "############################################################                \n",
    "\n",
    "\n",
    "# -------------------------------------------------------- #\n",
    "\n",
    "\n",
    "############################################################\n",
    "                    Start of Second Section                    \n",
    "############################################################\n",
    "def build_table_metadata_snapshot(\n",
    "    fq_table: str,\n",
    "    uc_metadata: Dict[str, List[Dict[str, Any]]],\n",
    "    describe_rows: List[Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    catalog, schema, table = parse_fq_table(fq_table)\n",
    "    # Table tags\n",
    "    table_tags = {row[\"tag_name\"]: row[\"tag_value\"] for row in uc_metadata.get(\"table_tags\", [])}\n",
    "    # Table properties, owner, comment\n",
    "    details = extract_table_details(describe_rows)\n",
    "    # Table check constraints (if present in table_properties, or elsewhere)\n",
    "    table_check_constraints = {\n",
    "        k: {\"name\": k, \"expression\": v}\n",
    "        for k, v in details.get(\"table_properties\", {}).items()\n",
    "        if k.startswith(\"delta.constraints\")\n",
    "    }\n",
    "    # Row filters\n",
    "    row_filters = [\n",
    "        {\"filter_name\": row[\"filter_name\"], \"target_columns\": row[\"target_columns\"]}\n",
    "        for row in uc_metadata.get(\"row_filters\", [])\n",
    "    ]\n",
    "    # Partition columns\n",
    "    partitioned_by = extract_partitioned_by(describe_rows)\n",
    "    # Constraints\n",
    "    constraints = extract_constraints(describe_rows)\n",
    "    # Primary key: from constraints\n",
    "    pk = []\n",
    "    for c in constraints:\n",
    "        if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "            m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "            if m:\n",
    "                pk = [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "    # Columns (by index)\n",
    "    columns_raw = extract_columns(describe_rows)\n",
    "    # Column tags (merge by column name)\n",
    "    col_tag_lookup = {}\n",
    "    for row in uc_metadata.get(\"column_tags\", []):\n",
    "        col = row[\"column_name\"]\n",
    "        if col not in col_tag_lookup:\n",
    "            col_tag_lookup[col] = {}\n",
    "        col_tag_lookup[col][row[\"tag_name\"]] = row[\"tag_value\"]\n",
    "    # Column check constraints (by constraint_column_usage)\n",
    "    col_constraint_lookup = {}\n",
    "    for row in uc_metadata.get(\"constraint_column_usage\", []):\n",
    "        col = row[\"column_name\"]\n",
    "        cons = row[\"constraint_name\"]\n",
    "        if col not in col_constraint_lookup:\n",
    "            col_constraint_lookup[col] = {}\n",
    "        col_constraint_lookup[col][cons] = {\"name\": cons}  # Expression requires deeper parsing if needed\n",
    "\n",
    "    # Build columns dictionary by position (1-based, as in your spec)\n",
    "    columns = {}\n",
    "    for idx, col in enumerate(columns_raw, start=1):\n",
    "        colname = col[\"name\"]\n",
    "        columns[idx] = {\n",
    "            \"column_name\": colname,\n",
    "            \"datatype\": col[\"datatype\"],\n",
    "            \"comment\": col[\"comment\"],\n",
    "            \"nullable\": None,  # Could be extracted if needed\n",
    "            \"masking_rule\": None,  # Could be extracted if needed\n",
    "            \"column_tags\": col_tag_lookup.get(colname, {}),\n",
    "            \"column_check_constraints\": col_constraint_lookup.get(colname, {}),\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        \"table\": {\n",
    "            \"fully_qualified_name\": fq_table,\n",
    "            \"catalog\": catalog,\n",
    "            \"schema\": schema,\n",
    "            \"table\": table,\n",
    "            \"owner\": details.get(\"owner\", \"\"),\n",
    "            \"comment\": details.get(\"comment\", \"\"),\n",
    "            \"table_properties\": details.get(\"table_properties\", {}),\n",
    "            \"table_tags\": table_tags,\n",
    "            \"table_check_constraints\": table_check_constraints,\n",
    "            \"row_filters\": row_filters,\n",
    "            \"partitioned_by\": partitioned_by,\n",
    "            \"primary_key\": pk,\n",
    "            \"columns\": columns,\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# --- Usage Example ---\n",
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "uc_metadata = get_metadata_snapshot(spark, table_name)\n",
    "describe_rows = get_describe_rows(spark, table_name)\n",
    "\n",
    "snapshot = build_table_metadata_snapshot(table_name, uc_metadata, describe_rows)\n",
    "import pprint; pprint.pprint(snapshot, width=120)\n",
    "\n",
    "############################################################\n",
    "                    End of Second Section                    \n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce092be-7dfe-4fe4-b3b4-d7ca56f9295d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class TableSnapshot:\n",
    "    SNAPSHOT_QUERIES = {\n",
    "        \"table_tags\": {\n",
    "            \"table\": \"system.information_schema.table_tags\",\n",
    "            \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"tag_name\", \"tag_value\"],\n",
    "            \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"column_tags\": {\n",
    "            \"table\": \"system.information_schema.column_tags\",\n",
    "            \"columns\": [\"catalog_name\", \"schema_name\", \"table_name\", \"column_name\", \"tag_name\", \"tag_value\"],\n",
    "            \"where_keys\": [(\"catalog_name\", 0), (\"schema_name\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"row_filters\": {\n",
    "            \"table\": \"system.information_schema.row_filters\",\n",
    "            \"columns\": [\"table_catalog\", \"table_schema\", \"table_name\", \"filter_name\", \"target_columns\"],\n",
    "            \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"constraint_table_usage\": {\n",
    "            \"table\": \"system.information_schema.constraint_table_usage\",\n",
    "            \"columns\": [\"constraint_catalog\", \"constraint_schema\", \"constraint_name\"],\n",
    "            \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "        \"constraint_column_usage\": {\n",
    "            \"table\": \"system.information_schema.constraint_column_usage\",\n",
    "            \"columns\": [\"column_name\", \"constraint_name\"],\n",
    "            \"where_keys\": [(\"table_catalog\", 0), (\"table_schema\", 1), (\"table_name\", 2)],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, spark: SparkSession, fq_table: str):\n",
    "        self.spark = spark\n",
    "        self.fq_table = fq_table\n",
    "        self.catalog, self.schema, self.table = self._parse_fq_table(fq_table)\n",
    "\n",
    "    def _parse_fq_table(self, fq_table: str):\n",
    "        parts = fq_table.split(\".\")\n",
    "        if len(parts) != 3:\n",
    "            raise ValueError(\"Expected format: catalog.schema.table\")\n",
    "        return parts[0], parts[1], parts[2]\n",
    "\n",
    "    def _build_metadata_sql(self, kind: str) -> str:\n",
    "        config = self.SNAPSHOT_QUERIES[kind]\n",
    "        table_vars = [self.catalog, self.schema, self.table]\n",
    "        where_clauses = [\n",
    "            f\"{col_name} = '{table_vars[idx]}'\"\n",
    "            for col_name, idx in config[\"where_keys\"]\n",
    "        ]\n",
    "        columns = \", \".join(config[\"columns\"])\n",
    "        return f\"SELECT {columns} FROM {config['table']} WHERE {' AND '.join(where_clauses)}\"\n",
    "\n",
    "    def _get_metadata_snapshot(self) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        results = {}\n",
    "        for kind in self.SNAPSHOT_QUERIES:\n",
    "            try:\n",
    "                sql = self._build_metadata_sql(kind)\n",
    "                df = self.spark.sql(sql)\n",
    "                results[kind] = [row.asDict() for row in df.collect()]\n",
    "            except Exception as e:\n",
    "                results[kind] = []\n",
    "        return results\n",
    "\n",
    "    def _get_describe_rows(self) -> List[Dict[str, Any]]:\n",
    "        sql = f\"DESCRIBE EXTENDED {self.fq_table}\"\n",
    "        df = self.spark.sql(sql)\n",
    "        return [row.asDict() for row in df.collect()]\n",
    "\n",
    "    def _extract_columns(self, describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        columns = []\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "            comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else \"\"\n",
    "            if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "                if col_name == \"# Partition Information\":\n",
    "                    break\n",
    "                continue\n",
    "            columns.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comment if comment.upper() != \"NULL\" else \"\",\n",
    "            })\n",
    "        return columns\n",
    "\n",
    "    def _extract_partitioned_by(self, describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "        collecting = False\n",
    "        partition_cols = []\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            if col_name == \"# Partition Information\":\n",
    "                collecting = True\n",
    "                continue\n",
    "            if collecting:\n",
    "                if not col_name or col_name.startswith(\"#\"):\n",
    "                    break\n",
    "                if col_name != \"# col_name\":\n",
    "                    partition_cols.append(col_name)\n",
    "        return partition_cols\n",
    "\n",
    "    def _extract_table_details(self, describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        details = {}\n",
    "        table_properties = {}\n",
    "        in_details = False\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "            if col_name == \"# Detailed Table Information\":\n",
    "                in_details = True\n",
    "                continue\n",
    "            if in_details:\n",
    "                if not col_name or col_name.startswith(\"#\"):\n",
    "                    break\n",
    "                if col_name == \"Owner\":\n",
    "                    details[\"owner\"] = data_type\n",
    "                elif col_name == \"Comment\":\n",
    "                    details[\"comment\"] = data_type\n",
    "                elif col_name == \"Table Properties\":\n",
    "                    for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                        if \"=\" in prop:\n",
    "                            k, v = prop.split(\"=\", 1)\n",
    "                            table_properties[k.strip()] = v.strip()\n",
    "        details[\"table_properties\"] = table_properties\n",
    "        return details\n",
    "\n",
    "    def _extract_constraints(self, describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "        constraints = []\n",
    "        in_constraints = False\n",
    "        for row in describe_rows:\n",
    "            col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "            data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "            if col_name == \"# Constraints\":\n",
    "                in_constraints = True\n",
    "                continue\n",
    "            if in_constraints:\n",
    "                if not col_name or col_name.startswith(\"#\"):\n",
    "                    break\n",
    "                if col_name and data_type:\n",
    "                    constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "        return constraints\n",
    "\n",
    "    def _build_columns(self, columns_raw: List[Dict[str, Any]], col_tags: Dict[str, Dict[str, Any]], col_checks: Dict[str, Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n",
    "        columns = {}\n",
    "        for idx, col in enumerate(columns_raw, start=1):\n",
    "            name = col[\"name\"]\n",
    "            columns[idx] = {\n",
    "                \"name\": name,\n",
    "                \"datatype\": col[\"datatype\"],\n",
    "                \"nullable\": None,  # Could be filled with extended logic\n",
    "                \"active\": True,\n",
    "                \"comment\": col.get(\"comment\", \"\"),\n",
    "                \"tags\": col_tags.get(name, {}),\n",
    "                \"column_masking_rule\": \"\",  # No masking in snapshot, set empty\n",
    "                \"column_check_constraints\": col_checks.get(name, {}),\n",
    "            }\n",
    "        return columns\n",
    "\n",
    "    def build_table_metadata_dict(self) -> Dict[str, Any]:\n",
    "        # Pull data\n",
    "        uc_metadata = self._get_metadata_snapshot()\n",
    "        describe_rows = self._get_describe_rows()\n",
    "\n",
    "        catalog, schema, table = self.catalog, self.schema, self.table\n",
    "\n",
    "        # Parse table tags\n",
    "        table_tags = {row[\"tag_name\"]: row[\"tag_value\"] for row in uc_metadata.get(\"table_tags\", [])}\n",
    "\n",
    "        # Extract table details\n",
    "        details = self._extract_table_details(describe_rows)\n",
    "\n",
    "        # Extract table check constraints (using table properties keys starting with delta.constraints)\n",
    "        table_check_constraints = {\n",
    "            k: {\"name\": k, \"expression\": v}\n",
    "            for k, v in details.get(\"table_properties\", {}).items()\n",
    "            if k.startswith(\"delta.constraints\")\n",
    "        }\n",
    "\n",
    "        # Row filters\n",
    "        row_filters = {}\n",
    "        for row in uc_metadata.get(\"row_filters\", []):\n",
    "            fname = row.get(\"filter_name\")\n",
    "            if fname:\n",
    "                row_filters[fname] = {\n",
    "                    \"name\": fname,\n",
    "                    \"expression\": row.get(\"target_columns\", \"\")\n",
    "                }\n",
    "\n",
    "        # Partition columns\n",
    "        partitioned_by = self._extract_partitioned_by(describe_rows)\n",
    "\n",
    "        # Constraints\n",
    "        constraints = self._extract_constraints(describe_rows)\n",
    "\n",
    "        # Primary key from constraints (parse SQL text)\n",
    "        pk = []\n",
    "        for c in constraints:\n",
    "            if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "                m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "                if m:\n",
    "                    pk = [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "\n",
    "        # Raw columns\n",
    "        columns_raw = self._extract_columns(describe_rows)\n",
    "\n",
    "        # Build column tags lookup\n",
    "        col_tag_lookup = {}\n",
    "        for row in uc_metadata.get(\"column_tags\", []):\n",
    "            col = row[\"column_name\"]\n",
    "            if col not in col_tag_lookup:\n",
    "                col_tag_lookup[col] = {}\n",
    "            col_tag_lookup[col][row[\"tag_name\"]] = row[\"tag_value\"]\n",
    "\n",
    "        # Build column check constraints lookup\n",
    "        col_constraint_lookup = {}\n",
    "        for row in uc_metadata.get(\"constraint_column_usage\", []):\n",
    "            col = row[\"column_name\"]\n",
    "            cons = row[\"constraint_name\"]\n",
    "            if col not in col_constraint_lookup:\n",
    "                col_constraint_lookup[col] = {}\n",
    "            col_constraint_lookup[col][cons] = {\"name\": cons}  # no expression parsing here\n",
    "\n",
    "        # Compose columns dict keyed by 1-based index\n",
    "        columns = self._build_columns(columns_raw, col_tag_lookup, col_constraint_lookup)\n",
    "\n",
    "        return {\n",
    "            \"full_table_name\": self.fq_table,\n",
    "            \"catalog\": catalog,\n",
    "            \"schema\": schema,\n",
    "            \"table\": table,\n",
    "            \"primary_key\": pk,\n",
    "            \"foreign_keys\": self._get_foreign_keys(uc_metadata),\n",
    "            \"unique_keys\": self._get_unique_keys(uc_metadata),\n",
    "            \"partitioned_by\": partitioned_by,\n",
    "            \"tags\": table_tags,\n",
    "            \"row_filters\": row_filters,\n",
    "            \"table_check_constraints\": table_check_constraints,\n",
    "            \"table_properties\": details.get(\"table_properties\", {}),\n",
    "            \"comment\": details.get(\"comment\", \"\"),\n",
    "            \"owner\": details.get(\"owner\", \"\"),\n",
    "            \"columns\": columns,\n",
    "        }\n",
    "\n",
    "    def _get_foreign_keys(self, uc_metadata: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Could be enhanced to parse foreign keys if available from metadata\n",
    "        return {}\n",
    "\n",
    "    def _get_unique_keys(self, uc_metadata: Dict[str, List[Dict[str, Any]]]) -> List[List[str]]:\n",
    "        # Could be enhanced to parse unique keys if available from metadata\n",
    "        return []\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "# table_snapshot = TableSnapshot(spark, \"dq_dev.lmg_sandbox.config_driven_table_example\")\n",
    "# metadata_dict = table_snapshot.build_table_metadata_dict()\n",
    "# pprint.pprint(metadata_dict, width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef7474e-ea47-463b-9203-87a99795ed61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "table_snapshot = TableSnapshot(spark, \"dq_dev.lmg_sandbox.config_driven_table_example\")\n",
    "metadata_dict = table_snapshot.build_table_metadata_dict()\n",
    "print(metadata_dict)\n",
    "#pprint.pprint(metadata_dict, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1a5465-81e6-4795-ab8c-7bd766bfaec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffdef1a1-4dff-4c17-b696-dd71495f2915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class TableSchemaConfig:\n",
    "    \"\"\"\n",
    "    Loader for YAML DDL config files. Exposes all config blocks with a clean API.\n",
    "    Handles dynamic env, catalog suffixes, and nested constraints/keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str, env: Optional[str] = None):\n",
    "        self.config_path = config_path\n",
    "        self._env = env\n",
    "        self._config: Dict[str, Any] = {}\n",
    "        self.load_config()\n",
    "\n",
    "    def load_config(self) -> None:\n",
    "        try:\n",
    "            with open(self.config_path, \"r\") as f:\n",
    "                self._config = yaml.safe_load(f)\n",
    "        except (FileNotFoundError, yaml.YAMLError) as e:\n",
    "            raise ValueError(f\"Error loading YAML configuration from {self.config_path}: {e}\")\n",
    "\n",
    "    @property\n",
    "    def catalog(self) -> str:\n",
    "        return self._config.get(\"catalog\", \"\")\n",
    "\n",
    "    @property\n",
    "    def schema(self) -> str:\n",
    "        return self._config.get(\"schema\", \"\")\n",
    "\n",
    "    @property\n",
    "    def table(self) -> str:\n",
    "        return self._config.get(\"table\", \"\")\n",
    "\n",
    "    @property\n",
    "    def env(self) -> Optional[str]:\n",
    "        return self._env\n",
    "\n",
    "    @property\n",
    "    def full_table_name(self) -> str:\n",
    "        cat = self.catalog.strip()\n",
    "        sch = self.schema.strip()\n",
    "        tbl = self.table.strip()\n",
    "        env = self.env\n",
    "        if cat.endswith(\"_\") and env:\n",
    "            cat_full = f\"{cat}{env}\"\n",
    "        else:\n",
    "            cat_full = cat\n",
    "        return f\"{cat_full}.{sch}.{tbl}\"\n",
    "\n",
    "    @property\n",
    "    def owner(self) -> str:\n",
    "        return self._config.get(\"owner\", \"\")\n",
    "\n",
    "    @property\n",
    "    def tags(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"tags\", {})\n",
    "\n",
    "    @property\n",
    "    def properties(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"properties\", {})\n",
    "\n",
    "    @property\n",
    "    def table_comment(self) -> str:\n",
    "        return self.properties.get(\"comment\", \"\")\n",
    "\n",
    "    @property\n",
    "    def table_properties(self) -> Dict[str, Any]:\n",
    "        return self.properties.get(\"table_properties\", {})\n",
    "\n",
    "    @property\n",
    "    def primary_key(self) -> List[str]:\n",
    "        pk = self._config.get(\"primary_key\", [])\n",
    "        return pk if isinstance(pk, list) else [pk]\n",
    "\n",
    "    @property\n",
    "    def partitioned_by(self) -> List[str]:\n",
    "        pb = self._config.get(\"partitioned_by\", [])\n",
    "        return pb if isinstance(pb, list) else [pb]\n",
    "\n",
    "    @property\n",
    "    def unique_keys(self) -> List[List[str]]:\n",
    "        return self._config.get(\"unique_keys\", [])\n",
    "\n",
    "    @property\n",
    "    def foreign_keys(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"foreign_keys\", {})\n",
    "\n",
    "    @property\n",
    "    def table_check_constraints(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"table_check_constraints\", {})\n",
    "\n",
    "    @property\n",
    "    def row_filters(self) -> Dict[str, Any]:\n",
    "        return self._config.get(\"row_filters\", {})\n",
    "\n",
    "    @property\n",
    "    def columns(self) -> List[Dict[str, Any]]:\n",
    "        cols_dict = self._config.get(\"columns\", {})\n",
    "        cols_dict_str = {str(k): v for k, v in cols_dict.items()}\n",
    "        sorted_keys = sorted(map(int, cols_dict_str.keys()))\n",
    "        return [cols_dict_str[str(k)] for k in sorted_keys]\n",
    "\n",
    "    def build_table_metadata_dict(self) -> Dict[str, Any]:\n",
    "        # Return dict with keys in the exact order you want — relies on Python 3.7+ insertion order preservation\n",
    "        result = {\n",
    "            \"full_table_name\": self.full_table_name,\n",
    "            \"catalog\": self.catalog,\n",
    "            \"schema\": self.schema,\n",
    "            \"table\": self.table,\n",
    "            \"primary_key\": self.primary_key if self.primary_key else [],\n",
    "            \"foreign_keys\": self.foreign_keys if self.foreign_keys else {},\n",
    "            \"unique_keys\": self.unique_keys if self.unique_keys else [],\n",
    "            \"partitioned_by\": self.partitioned_by if self.partitioned_by else [],\n",
    "            \"tags\": self.tags if self.tags else {},\n",
    "            \"row_filters\": self.row_filters if self.row_filters else {},\n",
    "            \"table_check_constraints\": self.table_check_constraints if self.table_check_constraints else {},\n",
    "            \"table_properties\": self.table_properties if self.table_properties else {},\n",
    "            \"comment\": self.table_comment,\n",
    "            \"owner\": self.owner,\n",
    "            \"columns\": {},\n",
    "        }\n",
    "\n",
    "        # Numbered columns with requested keys & order\n",
    "        for idx, col in enumerate(self.columns, 1):\n",
    "            result[\"columns\"][idx] = {\n",
    "                \"name\": col.get(\"name\", \"\"),\n",
    "                \"datatype\": col.get(\"datatype\", \"\"),\n",
    "                \"nullable\": col.get(\"nullable\", True),\n",
    "                \"active\": col.get(\"active\", True),\n",
    "                \"comment\": col.get(\"comment\", \"\"),\n",
    "                \"tags\": col.get(\"tags\", {}),\n",
    "                \"column_masking_rule\": col.get(\"column_masking_rule\", \"\"),\n",
    "                \"column_check_constraints\": col.get(\"column_check_constraints\", {}),\n",
    "            }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def describe(self) -> None:\n",
    "        # Helper for dev/test use only\n",
    "        print(f\"Table: {self.full_table_name}\")\n",
    "        print(f\"  Owner: {self.owner}\")\n",
    "        print(f\"  Tags: {self.tags}\")\n",
    "        print(f\"  Primary Key: {self.primary_key}\")\n",
    "        print(f\"  Partitioned By: {self.partitioned_by}\")\n",
    "        print(f\"  Unique Keys: {self.unique_keys}\")\n",
    "        print(f\"  Foreign Keys: {self.foreign_keys}\")\n",
    "        print(f\"  Table Check Constraints: {self.table_check_constraints}\")\n",
    "        print(f\"  Row Filters: {self.row_filters}\")\n",
    "        print(f\"  Table Properties: {self.table_properties}\")\n",
    "        print(f\"  Columns:\")\n",
    "        for i, col in enumerate(self.columns, 1):\n",
    "            print(\n",
    "                f\"    {i}: {col.get('name','')} ({col.get('datatype','')}, nullable={col.get('nullable', True)}) | \"\n",
    "                f\"comment={col.get('comment','')}, tags={col.get('tags',{})}, active={col.get('active', True)}\"\n",
    "            )\n",
    "            ccc = col.get(\"column_check_constraints\", {})\n",
    "            if ccc:\n",
    "                print(f\"      Column Check Constraints: {ccc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d73088-1cef-4fd6-b016-5bd0f6e822a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yaml_path = \"layker/resources/example.yaml\"\n",
    "\n",
    "cfg = TableSchemaConfig(yaml_path, env=\"dev\")\n",
    "table_meta = cfg.build_table_metadata_dict()\n",
    "print(table_meta)\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(table_meta, width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c36a67-31c9-4b0a-adac-28b7b99f311f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dev_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
