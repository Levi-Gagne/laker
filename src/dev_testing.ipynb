{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fda452b-510f-40c2-9eb6-d834c3b87ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dev Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88338-81f7-46c0-a5cd-128bf3232674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "describe_extended_query = \"\"\"\n",
    "DESCRIBE EXTENDED\n",
    "  {table_name}\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(describe_extended_query.format(table_name=table_name)).show(truncate=False, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d38569-7c57-41aa-adf0-84e920dbc503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_table_show(spark, fq_table: str):\n",
    "    df = spark.sql(f\"DESCRIBE TABLE EXTENDED {fq_table}\")\n",
    "    print(\"=== Raw DataFrame Schema ===\")\n",
    "    df.printSchema()\n",
    "    print(\"=== Raw DataFrame ===\")\n",
    "    df.show(truncate=False, n=100)\n",
    "    return df\n",
    "\n",
    "def dataframe_to_rowdicts(df):\n",
    "    rows = [row.asDict() for row in df.collect()]\n",
    "    print(\"=== Collected Rows ===\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "    return rows\n",
    "\n",
    "# Get DataFrame and show it\n",
    "df = describe_table_show(spark, \"dq_dev.lmg_sandbox.config_driven_table_example\")\n",
    "\n",
    "# Convert to Python list of dicts and show those\n",
    "rows = dataframe_to_rowdicts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a36b9b6-91b3-484c-93ba-ca47f2ffaf1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_describe_table(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert the output of DESCRIBE TABLE EXTENDED (rows as dicts)\n",
    "    into a nested dict matching YAML's structure.\n",
    "    \"\"\"\n",
    "    table_level = {}\n",
    "    columns = []\n",
    "    partitioned_by = []\n",
    "    constraints = []\n",
    "    table_properties = {}\n",
    "    owner = None\n",
    "    comment = None\n",
    "\n",
    "    # State tracking\n",
    "    mode = \"columns\"\n",
    "\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comm = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "\n",
    "        # Section transitions\n",
    "        if col_name == \"# Partition Information\":\n",
    "            mode = \"partition\"\n",
    "            continue\n",
    "        elif col_name == \"# Detailed Table Information\":\n",
    "            mode = \"details\"\n",
    "            continue\n",
    "        elif col_name == \"# Constraints\":\n",
    "            mode = \"constraints\"\n",
    "            continue\n",
    "        elif col_name.startswith(\"#\"):\n",
    "            mode = \"skip\"\n",
    "            continue\n",
    "\n",
    "        if mode == \"columns\" and col_name and not col_name.startswith(\"#\"):\n",
    "            columns.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comm if comm and comm.upper() != \"NULL\" else \"\",\n",
    "                # Placeholders for additional fields\n",
    "                \"nullable\": None,\n",
    "                \"tags\": {},\n",
    "                \"column_masking_rule\": None,\n",
    "                \"default_value\": None,\n",
    "                \"variable_value\": None,\n",
    "                \"allowed_values\": [],\n",
    "                \"column_check_constraints\": {},\n",
    "                \"active\": True,\n",
    "            })\n",
    "        elif mode == \"partition\" and col_name and col_name != \"# col_name\":\n",
    "            partitioned_by.append(col_name)\n",
    "        elif mode == \"details\":\n",
    "            if col_name == \"Catalog\":\n",
    "                table_level[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                table_level[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                table_level[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                owner = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                comment = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                # Parse table properties string into dict\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "            # Add more detail parsing as needed\n",
    "\n",
    "        elif mode == \"constraints\" and col_name and data_type:\n",
    "            constraints.append((col_name, data_type))\n",
    "\n",
    "    # Compose snapshot\n",
    "    table_level[\"owner\"] = owner\n",
    "    table_level[\"comment\"] = comment\n",
    "    table_level[\"partitioned_by\"] = partitioned_by\n",
    "    table_level[\"table_properties\"] = table_properties\n",
    "    # Parse out PK/unique from constraints\n",
    "    pk = []\n",
    "    for cname, dtype in constraints:\n",
    "        if dtype.startswith(\"PRIMARY KEY\"):\n",
    "            pk.append(dtype.split(\"`\")[1].replace(\"`\", \"\"))\n",
    "    table_level[\"primary_key\"] = pk\n",
    "\n",
    "    # Final structure\n",
    "    return {\n",
    "        \"table_level_values\": table_level,\n",
    "        \"column_level_values\": columns,\n",
    "    }\n",
    "\n",
    "# ---- Example usage ----\n",
    "snapshot = parse_describe_table(rows)\n",
    "from pprint import pprint\n",
    "pprint(snapshot)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dev_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
