{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fda452b-510f-40c2-9eb6-d834c3b87ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dev Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88338-81f7-46c0-a5cd-128bf3232674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "describe_extended_query = \"\"\"\n",
    "DESCRIBE EXTENDED\n",
    "  {table_name}\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(describe_extended_query.format(table_name=table_name)).show(truncate=False, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d38569-7c57-41aa-adf0-84e920dbc503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_table_show(spark, fq_table: str):\n",
    "    df = spark.sql(f\"DESCRIBE TABLE EXTENDED {fq_table}\")\n",
    "    print(\"=== Raw DataFrame Schema ===\")\n",
    "    df.printSchema()\n",
    "    print(\"=== Raw DataFrame ===\")\n",
    "    df.show(truncate=False, n=100)\n",
    "    return df\n",
    "\n",
    "def dataframe_to_rowdicts(df):\n",
    "    rows = [row.asDict() for row in df.collect()]\n",
    "    print(\"=== Collected Rows ===\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "    return rows\n",
    "\n",
    "# Get DataFrame and show it\n",
    "df = describe_table_show(spark, \"dq_dev.lmg_sandbox.config_driven_table_example\")\n",
    "\n",
    "# Convert to Python list of dicts and show those\n",
    "rows = dataframe_to_rowdicts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a36b9b6-91b3-484c-93ba-ca47f2ffaf1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_describe_table(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert the output of DESCRIBE TABLE EXTENDED (rows as dicts)\n",
    "    into a nested dict matching YAML's structure.\n",
    "    \"\"\"\n",
    "    table_level = {}\n",
    "    columns = []\n",
    "    partitioned_by = []\n",
    "    constraints = []\n",
    "    table_properties = {}\n",
    "    owner = None\n",
    "    comment = None\n",
    "\n",
    "    # State tracking\n",
    "    mode = \"columns\"\n",
    "\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comm = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "\n",
    "        # Section transitions\n",
    "        if col_name == \"# Partition Information\":\n",
    "            mode = \"partition\"\n",
    "            continue\n",
    "        elif col_name == \"# Detailed Table Information\":\n",
    "            mode = \"details\"\n",
    "            continue\n",
    "        elif col_name == \"# Constraints\":\n",
    "            mode = \"constraints\"\n",
    "            continue\n",
    "        elif col_name.startswith(\"#\"):\n",
    "            mode = \"skip\"\n",
    "            continue\n",
    "\n",
    "        if mode == \"columns\" and col_name and not col_name.startswith(\"#\"):\n",
    "            columns.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comm if comm and comm.upper() != \"NULL\" else \"\",\n",
    "                # Placeholders for additional fields\n",
    "                \"nullable\": None,\n",
    "                \"tags\": {},\n",
    "                \"column_masking_rule\": None,\n",
    "                \"default_value\": None,\n",
    "                \"variable_value\": None,\n",
    "                \"allowed_values\": [],\n",
    "                \"column_check_constraints\": {},\n",
    "                \"active\": True,\n",
    "            })\n",
    "        elif mode == \"partition\" and col_name and col_name != \"# col_name\":\n",
    "            partitioned_by.append(col_name)\n",
    "        elif mode == \"details\":\n",
    "            if col_name == \"Catalog\":\n",
    "                table_level[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                table_level[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                table_level[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                owner = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                comment = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                # Parse table properties string into dict\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "            # Add more detail parsing as needed\n",
    "\n",
    "        elif mode == \"constraints\" and col_name and data_type:\n",
    "            constraints.append((col_name, data_type))\n",
    "\n",
    "    # Compose snapshot\n",
    "    table_level[\"owner\"] = owner\n",
    "    table_level[\"comment\"] = comment\n",
    "    table_level[\"partitioned_by\"] = partitioned_by\n",
    "    table_level[\"table_properties\"] = table_properties\n",
    "    # Parse out PK/unique from constraints\n",
    "    pk = []\n",
    "    for cname, dtype in constraints:\n",
    "        if dtype.startswith(\"PRIMARY KEY\"):\n",
    "            pk.append(dtype.split(\"`\")[1].replace(\"`\", \"\"))\n",
    "    table_level[\"primary_key\"] = pk\n",
    "\n",
    "    # Final structure\n",
    "    return {\n",
    "        \"table_level_values\": table_level,\n",
    "        \"column_level_values\": columns,\n",
    "    }\n",
    "\n",
    "# ---- Example usage ----\n",
    "snapshot = parse_describe_table(rows)\n",
    "from pprint import pprint\n",
    "pprint(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af52c9a8-135f-4d78-b891-272994cdf9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def parse_fully_qualified_table(fq_table: str):\n",
    "    \"\"\"Split catalog.schema.table into (catalog, schema, table)\"\"\"\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected catalog.schema.table, got: {fq_table}\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def spark_sql_to_rows(spark: SparkSession, sql: str) -> List[dict]:\n",
    "    \"\"\"Runs a Spark SQL and returns the result as a list of dicts.\"\"\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def get_table_tags(spark: SparkSession, fq_table: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return all tags for the given table as a dict: {tag_name: tag_value, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT tag_name, tag_value\n",
    "        FROM system.information_schema.table_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    return {row['tag_name']: row['tag_value'] for row in rows}\n",
    "\n",
    "def get_column_tags(spark: SparkSession, fq_table: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Return all tags for each column in the table as:\n",
    "        {column_name: {tag_name: tag_value, ...}, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, tag_name, tag_value\n",
    "        FROM system.information_schema.column_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    col_tags = {}\n",
    "    for row in rows:\n",
    "        col = row['column_name']\n",
    "        tag = row['tag_name']\n",
    "        val = row['tag_value']\n",
    "        if col not in col_tags:\n",
    "            col_tags[col] = {}\n",
    "        col_tags[col][tag] = val\n",
    "    return col_tags\n",
    "\n",
    "def get_row_filters(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all row filters for a table as a list of dicts.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT filter_name, target_columns\n",
    "        FROM system.information_schema.row_filters\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_table_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints defined on the table (e.g. PK, Unique, FK).\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT constraint_name\n",
    "        FROM system.information_schema.constraint_table_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_column_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints for all columns on the table.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, constraint_name\n",
    "        FROM system.information_schema.constraint_column_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "  \n",
    "fq = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "\n",
    "  # 1. Table tags\n",
    "print(\"=== Table Tags ===\")\n",
    "print(get_table_tags(spark, fq))\n",
    "\n",
    "# 2. Column tags\n",
    "print(\"=== Column Tags ===\")\n",
    "print(get_column_tags(spark, fq))\n",
    "\n",
    "# 3. Row filters\n",
    "print(\"=== Row Filters ===\")\n",
    "print(get_row_filters(spark, fq))\n",
    "\n",
    "# 4. Table constraints\n",
    "print(\"=== Table Constraints ===\")\n",
    "print(get_constraint_table_usage(spark, fq))\n",
    "\n",
    "# 5. Column constraints\n",
    "print(\"=== Column Constraints ===\")\n",
    "print(get_constraint_column_usage(spark, fq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1797ee7c-649a-4b4e-b21e-e033a6573b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Spark SQL helpers ---\n",
    "def spark_sql_to_df(spark: SparkSession, sql: str):\n",
    "    \"\"\"\n",
    "    Run Spark SQL and return the DataFrame. Logs error, re-raises on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.sql(sql)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] spark_sql_to_df failed: {e}\\nSQL: {sql}\")\n",
    "        raise\n",
    "\n",
    "def spark_sql_to_rows(spark: SparkSession, sql: str):\n",
    "    \"\"\"\n",
    "    Run Spark SQL and return results as list of dicts. Logs error, re-raises on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.sql(sql)\n",
    "        return [row.asDict() for row in df.collect()]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] spark_sql_to_rows failed: {e}\\nSQL: {sql}\")\n",
    "        raise\n",
    "\n",
    "# --- Table introspection ---\n",
    "def describe_table_to_rows(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    sql = f\"DESCRIBE TABLE EXTENDED {fq_table}\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def extract_columns(spark: SparkSession, fq_table: str) -> List[Dict[str, Any]]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    cols = []\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comm = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "        if col_name and not col_name.startswith(\"#\"):\n",
    "            cols.append({\n",
    "                \"name\": col_name,\n",
    "                \"datatype\": data_type,\n",
    "                \"comment\": comm if comm and comm.upper() != \"NULL\" else \"\",\n",
    "                \"nullable\": None,\n",
    "                \"tags\": {},\n",
    "                \"column_masking_rule\": None,\n",
    "                \"default_value\": None,\n",
    "                \"variable_value\": None,\n",
    "                \"allowed_values\": [],\n",
    "                \"column_check_constraints\": {},\n",
    "                \"active\": True,\n",
    "            })\n",
    "    return cols\n",
    "\n",
    "def extract_partitioned_by(spark: SparkSession, fq_table: str) -> List[str]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        elif col_name.startswith(\"#\") and collecting:\n",
    "            break\n",
    "        elif collecting and col_name and col_name != \"# col_name\":\n",
    "            partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_details(spark: SparkSession, fq_table: str) -> Dict[str, Any]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    details = {}\n",
    "    props = {}\n",
    "    owner = None\n",
    "    comment = None\n",
    "    in_details = False\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Catalog\":\n",
    "                details[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                details[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                details[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                owner = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                comment = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        props[k.strip()] = v.strip()\n",
    "    details[\"owner\"] = owner\n",
    "    details[\"comment\"] = comment\n",
    "    details[\"table_properties\"] = props\n",
    "    return details\n",
    "\n",
    "def extract_constraints(spark: SparkSession, fq_table: str) -> List[Tuple[str, str]]:\n",
    "    rows = describe_table_to_rows(spark, fq_table)\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append((col_name, data_type))\n",
    "    return constraints\n",
    "\n",
    "def extract_primary_key(spark: SparkSession, fq_table: str) -> List[str]:\n",
    "    constraints = extract_constraints(spark, fq_table)\n",
    "    pk = []\n",
    "    for cname, dtype in constraints:\n",
    "        if \"PRIMARY KEY\" in dtype:\n",
    "            m = re.search(r\"\\((.*?)\\)\", dtype)\n",
    "            if m:\n",
    "                cols = [c.strip().replace(\"`\", \"\") for c in m.group(1).split(\",\")]\n",
    "                pk += cols\n",
    "    return pk\n",
    "\n",
    "def parse_describe_table(spark: SparkSession, fq_table: str) -> Dict[str, Any]:\n",
    "    details = extract_details(spark, fq_table)\n",
    "    columns = extract_columns(spark, fq_table)\n",
    "    partitioned_by = extract_partitioned_by(spark, fq_table)\n",
    "    pk = extract_primary_key(spark, fq_table)\n",
    "    details[\"partitioned_by\"] = partitioned_by\n",
    "    details[\"primary_key\"] = pk\n",
    "    return {\n",
    "        \"table_level_values\": details,\n",
    "        \"column_level_values\": columns,\n",
    "    }\n",
    "\n",
    "# ---- TEST ALL FUNCTIONS ----\n",
    "\n",
    "fq_table = \"dq_dev.lmg_sandbox.config_driven_table_example\"   # <--- update as needed\n",
    "\n",
    "print(\"==== COLUMNS ====\")\n",
    "print(extract_columns(spark, fq_table))\n",
    "\n",
    "print(\"==== PARTITIONED BY ====\")\n",
    "print(extract_partitioned_by(spark, fq_table))\n",
    "\n",
    "print(\"==== DETAILS ====\")\n",
    "print(extract_details(spark, fq_table))\n",
    "\n",
    "print(\"==== CONSTRAINTS ====\")\n",
    "print(extract_constraints(spark, fq_table))\n",
    "\n",
    "print(\"==== PRIMARY KEY ====\")\n",
    "print(extract_primary_key(spark, fq_table))\n",
    "\n",
    "print(\"==== FULL SNAPSHOT ====\")\n",
    "from pprint import pprint\n",
    "pprint(parse_describe_table(spark, fq_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cca53e-30dd-462d-bff5-714ebc28ae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_table_to_rows(spark, fq_table: str):\n",
    "    \"\"\"Run DESCRIBE TABLE EXTENDED and return rows as list of dicts.\"\"\"\n",
    "    df = spark.sql(f\"DESCRIBE TABLE EXTENDED {fq_table}\")\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "def extract_columns(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract just the column definitions from DESCRIBE output.\"\"\"\n",
    "    columns = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        comment = (row.get(\"comment\") or \"\").strip() if row.get(\"comment\") else None\n",
    "\n",
    "        # Columns section (stops at # Partition Info)\n",
    "        if col_name == \"\" or col_name.startswith(\"#\"):\n",
    "            if col_name == \"# Partition Information\":\n",
    "                break\n",
    "            continue\n",
    "        columns.append({\n",
    "            \"name\": col_name,\n",
    "            \"datatype\": data_type,\n",
    "            \"comment\": comment if comment and comment.upper() != \"NULL\" else \"\",\n",
    "            # only these three fields are real here\n",
    "        })\n",
    "    return columns\n",
    "\n",
    "def extract_partitioned_by(describe_rows: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"Extract partition columns (if any).\"\"\"\n",
    "    collecting = False\n",
    "    partition_cols = []\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        if col_name == \"# Partition Information\":\n",
    "            collecting = True\n",
    "            continue\n",
    "        if collecting:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name != \"# col_name\":\n",
    "                partition_cols.append(col_name)\n",
    "    return partition_cols\n",
    "\n",
    "def extract_table_details(describe_rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract catalog, schema, table, owner, comment, table_properties only.\"\"\"\n",
    "    details = {}\n",
    "    table_properties = {}\n",
    "    in_details = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Detailed Table Information\":\n",
    "            in_details = True\n",
    "            continue\n",
    "        if in_details:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name == \"Catalog\":\n",
    "                details[\"catalog\"] = data_type\n",
    "            elif col_name == \"Database\":\n",
    "                details[\"schema\"] = data_type\n",
    "            elif col_name == \"Table\":\n",
    "                details[\"table\"] = data_type\n",
    "            elif col_name == \"Owner\":\n",
    "                details[\"owner\"] = data_type\n",
    "            elif col_name == \"Comment\":\n",
    "                details[\"comment\"] = data_type\n",
    "            elif col_name == \"Table Properties\":\n",
    "                for prop in data_type.strip(\"[]\").split(\",\"):\n",
    "                    if \"=\" in prop:\n",
    "                        k, v = prop.split(\"=\", 1)\n",
    "                        table_properties[k.strip()] = v.strip()\n",
    "    details[\"table_properties\"] = table_properties\n",
    "    return details\n",
    "\n",
    "def extract_constraints(describe_rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extract table constraints as dicts: name/type\"\"\"\n",
    "    constraints = []\n",
    "    in_constraints = False\n",
    "    for row in describe_rows:\n",
    "        col_name = (row.get(\"col_name\") or \"\").strip()\n",
    "        data_type = (row.get(\"data_type\") or \"\").strip()\n",
    "        if col_name == \"# Constraints\":\n",
    "            in_constraints = True\n",
    "            continue\n",
    "        if in_constraints:\n",
    "            if not col_name or col_name.startswith(\"#\"):\n",
    "                break\n",
    "            if col_name and data_type:\n",
    "                constraints.append({\"name\": col_name, \"type\": data_type})\n",
    "    return constraints\n",
    "\n",
    "def extract_primary_key(describe_rows: List[Dict[str, Any]]) -> Optional[List[str]]:\n",
    "    \"\"\"Find and parse PRIMARY KEY constraint, if present.\"\"\"\n",
    "    cons = extract_constraints(describe_rows)\n",
    "    for c in cons:\n",
    "        if \"PRIMARY KEY\" in c[\"type\"]:\n",
    "            m = re.search(r\"\\((.*?)\\)\", c[\"type\"])\n",
    "            if m:\n",
    "                return [col.strip().replace(\"`\", \"\") for col in m.group(1).split(\",\")]\n",
    "    return None\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "from typing import List, Dict\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def parse_fully_qualified_table(fq_table: str):\n",
    "    \"\"\"Split catalog.schema.table into (catalog, schema, table)\"\"\"\n",
    "    parts = fq_table.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected catalog.schema.table, got: {fq_table}\")\n",
    "    return parts[0], parts[1], parts[2]\n",
    "\n",
    "def spark_sql_to_rows(spark: SparkSession, sql: str) -> List[dict]:\n",
    "    \"\"\"Runs a Spark SQL and returns the result as a list of dicts.\"\"\"\n",
    "    df = spark.sql(sql)\n",
    "    return [row.asDict() for row in df.collect()]\n",
    "\n",
    "def get_table_tags(spark: SparkSession, fq_table: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return all tags for the given table as a dict: {tag_name: tag_value, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT tag_name, tag_value\n",
    "        FROM system.information_schema.table_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    return {row['tag_name']: row['tag_value'] for row in rows}\n",
    "\n",
    "def get_column_tags(spark: SparkSession, fq_table: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Return all tags for each column in the table as:\n",
    "        {column_name: {tag_name: tag_value, ...}, ...}\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, tag_name, tag_value\n",
    "        FROM system.information_schema.column_tags\n",
    "        WHERE catalog_name = '{catalog}'\n",
    "          AND schema_name = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    rows = spark_sql_to_rows(spark, sql)\n",
    "    col_tags = {}\n",
    "    for row in rows:\n",
    "        col = row['column_name']\n",
    "        tag = row['tag_name']\n",
    "        val = row['tag_value']\n",
    "        if col not in col_tags:\n",
    "            col_tags[col] = {}\n",
    "        col_tags[col][tag] = val\n",
    "    return col_tags\n",
    "\n",
    "def get_row_filters(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all row filters for a table as a list of dicts.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT filter_name, target_columns\n",
    "        FROM system.information_schema.row_filters\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_table_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints defined on the table (e.g. PK, Unique, FK).\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT constraint_name\n",
    "        FROM system.information_schema.constraint_table_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)\n",
    "\n",
    "def get_constraint_column_usage(spark: SparkSession, fq_table: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Return all constraints for all columns on the table.\n",
    "    \"\"\"\n",
    "    catalog, schema, table = parse_fully_qualified_table(fq_table)\n",
    "    sql = f\"\"\"\n",
    "        SELECT column_name, constraint_name\n",
    "        FROM system.information_schema.constraint_column_usage\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND table_name = '{table}'\n",
    "    \"\"\"\n",
    "    return spark_sql_to_rows(spark, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a566741-5773-401d-9d47-10c62829365c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Run these in order to see exactly what you get ---\n",
    "fq = \"dq_dev.lmg_sandbox.config_driven_table_example\"\n",
    "rows = describe_table_to_rows(spark, fq)\n",
    "\n",
    "print(\"\\n--- Columns ---\")\n",
    "print(extract_columns(rows))\n",
    "\n",
    "print(\"\\n--- Partitioned By ---\")\n",
    "print(extract_partitioned_by(rows))\n",
    "\n",
    "print(\"\\n--- Table Details ---\")\n",
    "print(extract_table_details(rows))\n",
    "\n",
    "print(\"\\n--- Constraints ---\")\n",
    "print(extract_constraints(rows))\n",
    "\n",
    "print(\"\\n--- Primary Key ---\")\n",
    "print(extract_primary_key(rows))\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# 1. Table tags\n",
    "print(\"=== Table Tags ===\")\n",
    "print(get_table_tags(spark, fq))\n",
    "\n",
    "# 2. Column tags\n",
    "print(\"=== Column Tags ===\")\n",
    "print(get_column_tags(spark, fq))\n",
    "\n",
    "# 3. Row filters\n",
    "print(\"=== Row Filters ===\")\n",
    "print(get_row_filters(spark, fq))\n",
    "\n",
    "# 4. Table constraints\n",
    "print(\"=== Table Constraints ===\")\n",
    "print(get_constraint_table_usage(spark, fq))\n",
    "\n",
    "# 5. Column constraints\n",
    "print(\"=== Column Constraints ===\")\n",
    "print(get_constraint_column_usage(spark, fq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2672c61-5950-4942-895e-2f5923f0dab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dev_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
