LAYKER — Frequently Asked Questions
======================================================

This document presents the FAQ as an indented, table-of-contents style outline
so you can scan topics quickly and find the answer you need.

LEGEND
------
• Top-level sections are numbered 0..5
• Subsections use dotted numbering (e.g., 1.3)
• Bullets under each item summarize the key points
• Indentation indicates hierarchy

0. VERSION, COMPATIBILITY & ENVIRONMENT
   0.1 What Python and Spark versions are supported?
       - Python: 3.9+ (tested primarily on 3.10/3.11)
       - Spark: 3.3+ (Databricks Runtime 12.x+ or equivalent)
       - Core deps: pyspark, pyyaml
   0.2 Can I run this outside Databricks?
       - Yes, on any Spark 3.x cluster.
       - Some features assume Delta/Unity Catalog (e.g., REFRESH, Delta properties).
       - Serverless caveat: REFRESH TABLE is not supported on serverless compute.

1. USAGE & ORCHESTRATION
   1.1 How do I run Layker?
       - Python:
         > from layker.main import run_table_load
           run_table_load(yaml_path="path/to/table.yaml", env="prd", mode="apply")
       - CLI:
         > python -m layker path/to/table.yaml prd false apply [audit_log_table]
   1.2 Which modes exist and when do they exit?
       - validate  : validate YAML then exit (no cluster changes).
       - diff      : compute/print differences then exit (no apply).
       - apply/all : compute/apply differences; logs an audit row only on change.
   1.3 Does the tool write audit logs when there is no change?
       - No. We detect “no-op” diffs early and exit with a success message.
   1.4 How is the audit log table chosen?
       - Parameter `audit_log_table` accepts:
         • True  -> uses default YAML at: layker/resources/layker_audit.yaml
         • False -> disables audit logging
         • str   -> path to a YAML that defines the audit table
       - The YAML defines the table FQN and schema (Delta).
   1.5 What does the audit log row contain?
       - Keys: change_id, run_id, env, yaml_path, fqn, change_category (create/update),
         change_key (e.g., create-1 or create-2~update-5), differences (JSON),
         before_value (JSON), after_value (JSON), notes, created_at/by, updated_at/by
       - change_key is unique per (fqn, change_key) sequence and increments by table.
   1.6 Where can this run in a workflow?
       - Ad hoc notebook, scheduled job step, or after ETL writes to enforce metadata.
   1.7 Does the tool ever touch table data?
       - No. Only schema/metadata (DDL). Dropped columns are hidden; data persists
         in Delta until vacuumed (standard Delta semantics).

2. YAML STORAGE & MANAGEMENT
   2.1 Where should I store YAML files?
       - Recommended: in git with PR review (auditability).
       - Alternative: in a Databricks volume (operational agility).
   2.2 Can YAML live inside a notebook cell?
       - Discouraged for maintainability; use files for version control and reviews.
   2.3 How should I handle environments?
       - Use per-env files or suffix patterns (e.g., catalog endswith '_' + env).
       - Pass env to `run_table_load(..., env="dev")` to resolve catalog.

3. YAML FORMAT & VALIDATION
   3.1 Minimum required fields
       - Top-level: catalog, schema, table, columns
       - Columns indexed 1..N (continuous); each column needs name, datatype, nullable, active
   3.2 Additional supported metadata
       - primary_key, unique_keys, foreign_keys, partitioned_by
       - table_comment, table_properties, table_check_constraints, row_filters, tags, owner
   3.3 Comment & value sanitization
       - Column comments cannot include newline, carriage return, tab, or single quotes.
       - Text is sanitized to keep DDL safe and readable.
   3.4 Validation outcomes
       - Fails fast with a clear list of problems; exits before any changes.
       - On success, Layker builds a sanitized YAML snapshot and a full-table FQN.

4. DIFFERENCES & SCHEMA EVOLUTION
   4.1 What is considered a “change”?
       - The generated diff returns at most: { full_table_name, add, update, remove }.
       - If add/update/remove all empty → no change (Layker exits without logging).
   4.2 What schema evolution operations are detected?
       - Add column   : add.columns has an index ≥ 2 with a truthy “name”, and index 1 is not present.
       - Rename column: update.columns[idx].name is truthy.
       - Drop column  : remove.columns[idx].name is truthy.
       - Full create  : table_snapshot is None (not treated as evolution).
   4.3 What validation runs before applying evolution?
       - validate_differences() calls a pre-flight that checks required Delta properties:
         • delta.columnMapping.mode = name
         • delta.minReaderVersion   = 2
         • delta.minWriterVersion   = 5
       - If missing: prints an error and exits (no partial updates).
   4.4 What’s unsupported?
       - Changing column types, reordering columns, or mixing complex composite ops.
       - For a type change, create a new table or reload data with the new schema.

5. AUDIT LOG DETAILS
   5.1 When do we write to the audit log?
       - Only when a diff exists and we apply it (apply/all modes).
   5.2 What is change_key and how is it enforced?
       - Per-table sequence:
         • CREATE:  create-{n}
         • UPDATE:  create-{max_create}~update-{m}
       - Computed by counting prior rows for the same FQN; unique within (fqn, change_key).
   5.3 What JSON fields are stored?
       - differences: snapshot diff (cleaned dict); before_value and after_value are
         pretty-printed JSON snapshots of the table metadata.
   5.4 Any serverless caveats?
       - REFRESH TABLE is skipped with a warning on serverless compute since it is
         not supported. Snapshots are still taken.

END OF OUTLINE
