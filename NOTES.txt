So i tried pip installing and while it worked this was the ouput: 
Collecting layker
  Downloading layker-0.1.0-py3-none-any.whl.metadata (14 kB)
Collecting pyspark>=3.3.0 (from layker)
  Downloading pyspark-4.0.0.tar.gz (434.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.1/434.1 MB 20.8 MB/s eta 0:00:0000:0100:01
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: pyyaml>=5.4 in /databricks/python3/lib/python3.11/site-packages (from layker) (6.0)
Requirement already satisfied: typing-extensions>=4.0 in /databricks/python3/lib/python3.11/site-packages (from layker) (4.10.0)
Collecting py4j==0.10.9.9 (from pyspark>=3.3.0->layker)
  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)
Downloading layker-0.1.0-py3-none-any.whl (55 kB)
Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)
Building wheels for collected packages: pyspark
  Building wheel for pyspark (setup.py): started
  Building wheel for pyspark (setup.py): finished with status 'done'
  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741244 sha256=6f8df68a4cd2f4e1d829b6c3b0b2dd8077fb43c719e01f57fd13f6574dae2585
  Stored in directory: /home/spark-9bc7828e-b256-4e4f-ad18-ab/.cache/pip/wheels/91/e4/c1/3c917d48563ae77204dd185aa3da90da6a1a5526565296dadf
Successfully built pyspark
Installing collected packages: py4j, pyspark, layker
  Attempting uninstall: py4j
    Found existing installation: py4j 0.10.9.7
    Not uninstalling py4j at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9bc7828e-b256-4e4f-ad18-ab9a77afbc29
    Can't uninstall 'py4j'. No files were found to uninstall.
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
databricks-connect 15.4.12 requires py4j==0.10.9.7, but you have py4j 0.10.9.9 which is incompatible.
Successfully installed layker-0.1.0 py4j-0.10.9.9 pyspark-4.0.0
Note: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.



So on my databricks i already have access to pyspark, and so will 99 percent of people working in databricks, so to speed things up can we not have this package install pyspark if its already avalible? 


for example i tested this code without installing pyspark then whne i publish as a package and pip install it automatically does it
